{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis and Consumer Profiling\n",
    "\n",
    "This script includes cleaning, EDA, feature creation, and some preliminary analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib \n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food = pd.read_csv('./fastfood.csv', dtype=object, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152378, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping malformed data (invalid index)\n",
    "food.drop(list(food.loc[food['unique_code'] == 'Nobody should be too big to fail...'].index),\n",
    "          axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping duplicate entries\n",
    "food = food.drop_duplicates(subset='unique_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66504, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eliminated more than 50% of observations\n",
    "food.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping all rows with all null values\n",
    "food = food.drop(food[food.isnull().all(axis=1)].index[0], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropping all values that are not company related (only 3 observations)\n",
    "to_drop = []\n",
    "\n",
    "for row_num, val in enumerate(food['Company']):\n",
    "    if val[0] != '@':\n",
    "        to_drop.append(row_num)\n",
    "        \n",
    "food = food.drop(to_drop, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replacing strings with integers\n",
    "mapper = {'True': 1, 'False': 0}\n",
    "food['user_is_verified'] = food.user_is_verified.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filling nulls and converting data types\n",
    "food['retweet_count'] = food.retweet_count.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set to run Midnight and 5pm EST everyday, the times are in UTC, making EST\n",
    "food['time_tweeted'] = pd.to_datetime(food['time_tweeted']) - pd.Timedelta(hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food['favorite_count'] = food.favorite_count.astype(int)\n",
    "food['number_of_people_they_follow'] = food.number_of_people_they_follow.astype(int)\n",
    "food['number_of_user_tweets'] = food.number_of_user_tweets.astype(int)\n",
    "food['user_followers_count'] = food.user_followers_count.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a basic name category that isnt the handle\n",
    "\n",
    "mapper = {'@DennysDiner': 'Dennys', '@ChipotleTweets': 'Chipotle',\n",
    "         '@McDonalds': 'McDonalds', '@Wendys': 'Wendys', '@Starbucks':'Starbucks',\n",
    "         '@dunkindonuts':'Dunkin_Donuts', '@dominos': 'Dominos', '@shakeshack': 'Shake_Shack',\n",
    "         '@sonicdrivein': 'Sonic', '@wingstop': 'Wingstop', '@CrackerBarrel': 'Cracker_Barrel', \n",
    "         '@redrobinburgers': 'Red_Robin', '@Potbelly': 'Potbelly'}\n",
    "\n",
    "food['name'] = food.Company.map(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examined multiple methods of cleaning for sentiment data, only the final method runs now, but \n",
    "have kept my trial methods below (hashed out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shouldnt be removing stopwords before sentiment analysis:\n",
    "#http://www.lrec-conf.org/proceedings/lrec2014/pdf/292_Paper.pdf\n",
    "#testing different functions for preprocessing text for sentiment analysis\n",
    "\n",
    "\n",
    "def Text_Cleaner(text, tokens=False):\n",
    "    \"\"\"Takes text, eliminates URLS, replaces contractions, tokenizes, \n",
    "    removes company names, lower cases, removes calls to twitter handles, \n",
    "    returns a string, same as version 1, but only looking at words\"\"\"\n",
    "    text = re.sub(r'(https)[^\\s]+', '', text)\n",
    "    text = re.sub(r'can\\'t', 'can not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'don\\'t', 'do not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'isn\\'t', 'is not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'aren\\'t', 'are not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'wasn\\'t', 'was not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'weren\\'t', 'were not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'haven\\'t', 'have not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(rt|RT)', '', text)\n",
    "    text = re.sub(r'@[a-zA-Z0-9]+', '', text)\n",
    "    text = re.sub('#', '', text)\n",
    "    text = re.sub(r'(wtf)+\\b', 'what the fuck', text, flags=re.IGNORECASE)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    if tokens:\n",
    "        words = [word.lower() for word in words]\n",
    "        return words\n",
    "    return ' '.join(words)\n",
    "\n",
    "# def Text_Cleaner_version_1(text):\n",
    "#     \"\"\"Takes text, eliminates URLS, replaces contractions, tokenizes, \n",
    "#     removes company names, lower cases, removes calls to twitter handles, \n",
    "#     returns a string\"\"\"\n",
    "#     text = re.sub(r'(https)[^\\s]+', '', text)\n",
    "#     text = re.sub(r'\\b(rt|RT)', '', text)\n",
    "#     text = re.sub(r'@[a-zA-Z0-9]+', '', text)\n",
    "#     text = re.sub('#', '', text)\n",
    "#     return text\n",
    "\n",
    "#     LOOKING AT VARIOUS METHODS FOR PREPROCESSING FOR SENTIMENT ANALYSIS\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     words = tokenizer.tokenize(text)\n",
    "#     lower = [x.lower() for x in words]\n",
    "#     words = [word for word in words if word != 'rt']\n",
    "#     eliminator = [re.sub(r'(mcdon|dunki|denn|redro|sonic|starb|shakesh|domino|crackerb|chipot|wend)[a-z]+','',x)\n",
    "#                   for x in lower]\n",
    "#     return ' '.join(eliminator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Creating a test set of uncleaned data to check the value of the Text_Cleaner functions\n",
    "# food['text_sentiment_no_clean'] = food['text'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #converting text with version 1\n",
    "# food['text_sentiment_v1'] = food['text'].apply(Text_Cleaner_version_1)\n",
    "# #Calculating sentiment with TextBlob\n",
    "# food['sentiment_score_v1'] = food['text_sentiment_v1'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting text with version 2\n",
    "food['text_sentiment'] = food['text'].apply(Text_Cleaner)\n",
    "#Calculating sentiment with TextBlob\n",
    "food['sentiment_score'] = food['text_sentiment'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company\n",
       "@dominos            0.065029\n",
       "@dunkindonuts       0.094908\n",
       "@wingstop           0.097293\n",
       "@sonicdrivein       0.101714\n",
       "@McDonalds          0.106599\n",
       "@Wendys             0.108249\n",
       "@DennysDiner        0.113954\n",
       "@redrobinburgers    0.121663\n",
       "@ChipotleTweets     0.147069\n",
       "@CrackerBarrel      0.153147\n",
       "@Potbelly           0.162750\n",
       "@Starbucks          0.173199\n",
       "@shakeshack         0.184626\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at overall sentiment by company (version 1) (0 neutral, 1 positive, -1 negative)\n",
    "food.groupby('Company')['sentiment_score'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating sentiment dummy variables \n",
    "\n",
    "def dummy_maker(val):\n",
    "    \"\"\"Takes in a float and returns a dummy based on the value\n",
    "    to be used in pandas.apply\"\"\"\n",
    "    if val == 0:\n",
    "        return 0\n",
    "    elif val > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "food['sentiment_dummies'] = food['sentiment_score'].apply(dummy_maker)\n",
    "\n",
    "# food['sentiment_dummies_v2'] = food['sentiment_score_v2'].apply(dummy_maker)\n",
    "\n",
    "# food['sentiment_dummies_uncleaned'] = food['text_sentiment_no_clean'].apply(dummy_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    28738\n",
      " 0    27902\n",
      "-1     9860\n",
      "Name: sentiment_dummies, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print(food['sentiment_dummies_v1'].value_counts()) \n",
    "# print(food['sentiment_dummies_v2'].value_counts())\n",
    "print(food['sentiment_dummies'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @danshiin: @Starbucks @doolsetbangtan @SugasHero RAPLINE IS SHOOK https://t.co/FEzkl1tYKQ\n",
      "\n",
      "\n",
      "[1]\n",
      "Hungry to make a difference? In many Mid-Atlantic and Northeastern U.S. states, @chipotletweets is donating 50% of… https://t.co/qd6c82QxJr\n",
      "\n",
      "\n",
      "[0]\n",
      "so i’m about to have wingstop for the first time in my life.. don’t let me down @wingstop\n",
      "\n",
      "\n",
      "[-1]\n",
      "@sallykuchar @AAAArch @CurbedSF @DennysDiner I thought you'd recognize it.\n",
      "\n",
      "\n",
      "[0]\n",
      "RT @dominos: When you just can’t wait to dig in. https://t.co/KMgyrbhiLO\n",
      "\n",
      "\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#manually testing reliability of the different measures with a random subset\n",
    "random_numbers = list(np.random.randint(0, 62415, 5))\n",
    "for num, val in enumerate(food.iloc[random_numbers, :]['text']):\n",
    "    print(val)\n",
    "    print('\\n')\n",
    "    print(food.iloc[[num], :]['sentiment_dummies'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to quickly separate positive/negative tweets by company\n",
    "\n",
    "def negativity_formatter(company, hourly_rate=False):\n",
    "    \"\"\"This function accepts a company and returns either \n",
    "    the separate company, positive, negative dataframes (in that order)\n",
    "    OR the same order plus a dataframe of hourly rates, if hourly_rate = True\"\"\"\n",
    "    df = food.loc[food['name'] == company]\n",
    "    positive_df = df.loc[df['sentiment_dummies'] == 1]\n",
    "    negative_df = df.loc[df['sentiment_dummies'] == -1]\n",
    "    if not hourly_rate:\n",
    "        return df, positive_df, negative_df\n",
    "    else:\n",
    "        rate_df = pd.DataFrame()\n",
    "        rate_df['pos_count'] = positive_df.groupby(positive_df['time_tweeted'].dt.hour)['Company'].count()\n",
    "        rate_df['neg_count'] = negative_df.groupby(negative_df['time_tweeted'].dt.hour)['Company'].count()\n",
    "        rate_df['rate'] = rate_df['neg_count'] / (rate_df['pos_count'] + rate_df['neg_count'])\n",
    "        return rate_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43109774436090226"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#43% of tweets are retweets\n",
    "food.loc[food['text'].str[:2] == 'RT'].shape[0] / food.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a dummy for whether the tweet is a retweet or not\n",
    "retweets = []\n",
    "for val in food['text']:\n",
    "    if val[:2] == 'RT':\n",
    "        retweets.append(1)\n",
    "    else: \n",
    "        retweets.append(0)\n",
    "food['is_a_retweet'] = retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting rid of tweets without a user \n",
    "food = food.drop(list(food.loc[(food['user_name'].isnull())].index), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping regional affiliates\n",
    "associated_comps = []\n",
    "names = []\n",
    "for num, name in enumerate(food['user_name']):\n",
    "    test = re.findall(r'\\A(mcdon|dunki|redro|starbuc|shakesh|domino|crackerb)[a-z]+'\n",
    "                      , name, flags=re.IGNORECASE)\n",
    "    if test:\n",
    "        if name not in ['Dunkin Fails', 'Dunkin Kitti', \"McDonald's employee\"]:\n",
    "            associated_comps.append(num)\n",
    "            names.append(name)\n",
    "            \n",
    "food = food.drop(associated_comps, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    66441.000000\n",
       "mean         0.508662\n",
       "std          5.554575\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max        857.000000\n",
       "Name: favorite_count, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#favorite count is heavilly skewed, but keep for now\n",
    "food['favorite_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning to look at user profiles by company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#customer profiles still skewed by large means- looking at medians & means\n",
    "#creating a subset to examine the numeric characteristics of each company's customers\n",
    "individual_users = food.drop_duplicates(subset='user_name')\n",
    "\n",
    "customer_numeric_df = individual_users.groupby('name').agg({'favorite_count': ['mean', 'max'], \n",
    "                                      'number_of_people_they_follow': ['median', 'mean'],\n",
    "                                      'number_of_user_tweets': ['median', 'mean'],\n",
    "                                      'retweet_count': ['median', 'mean'],\n",
    "                                      'user_followers_count': ['median', 'mean'],\n",
    "                                       'user_is_verified': ['mean'],\n",
    "                                      'sentiment_score': ['mean'], \n",
    "                                      'is_a_retweet': 'mean',\n",
    "                                      'Company': 'count'})\n",
    "\n",
    "customer_numeric_df.columns = [' '.join(col).strip() for col in customer_numeric_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Not enough observations for Potbelly (684), RedRobin (1463), dropping\n",
    "food = food.drop(list(food.loc[(food['Company']== '@redrobinburgers')|\n",
    "                   (food['Company']== '@Potbelly')].index), axis=0)\n",
    "food = food.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MISSING VALUES FOR CERTAIN COMPANIES AT CERTAIN HOURS- hourly is flawed\n",
    "# negativity_by_comp = pd.DataFrame()\n",
    "# for val in food.name.unique().tolist():\n",
    "#     if val != 'McDonalds':\n",
    "#         values = list(negativity_formatter(val, hourly_rate=True)['rate'].values)\n",
    "#         negativity_by_comp[val] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food['day_date'] = food['time_tweeted'].dt.day\n",
    "food['weekday'] = food['time_tweeted'].dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning look at stock movements and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specifying trading days with sufficient information range\n",
    "stock_analysis = food.loc[(food['day_date'] > 11) & (food['day_date'] < 31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63463, 21)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_analysis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = pd.read_csv('./twitter_stocks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks.drop(['High', 'Low', 'Adj Close'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks['change'] = stocks['Close'] - stocks['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating datetime, and locating common trading dates\n",
    "stocks['Date'] = pd.to_datetime(stocks.Date)\n",
    "stocks['day'] = stocks['Date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks['day'] = stocks['Date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = stocks.loc[(stocks['day'] > 11) & (stocks['day'] < 31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making sure values match before merging\n",
    "stocks['Name'] = stocks.Name.str.replace('Shack Shack', 'Shake_Shack')\n",
    "stocks['Name'] = stocks.Name.str.replace('Cracker Barrel', 'Cracker_Barrel')\n",
    "stocks['Name'] = stocks.Name.str.replace('Dunkin Donuts', 'Dunkin_Donuts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grouping sentiments to merge\n",
    "stock_analysis = stock_analysis.groupby(['day_date', 'name'], as_index=False).agg(\n",
    "                                                    {'sentiment_score':'mean',\n",
    "                                                       'Company':'count'})\n",
    "\n",
    "stock_analysis.columns =  ['day_date', 'name', 'sentiment_score', 'num_observations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_stock = pd.merge(stock_analysis, stocks, left_on=['day_date', 'name'], right_on=['day', 'Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping redundant columns \n",
    "merged_stock.drop(['day', 'Name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_stock = merged_stock.sort_values(['name', 'day_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "day_date           -0.094365\n",
       "sentiment_score     1.000000\n",
       "num_observations   -0.013644\n",
       "Open               -0.014959\n",
       "Close              -0.012543\n",
       "Volume              0.145016\n",
       "Market Cap          0.089305\n",
       "change              0.102586\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#correlations between sentiment_score and various other indicators (volume/change/market_cap)\n",
    "merged_stock.corr()['sentiment_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Profiles of negative/positive sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set of english vocabulary\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lang_composition(company):\n",
    "    \"\"\"Takes a company name and returns the total number of words present, \n",
    "    the number of unique words present, and the number of those words that are in the English\n",
    "    language\"\"\"\n",
    "    all_tweets, positive, negative = negativity_formatter(company)\n",
    "    all_text = ' '\n",
    "    pos_text = ' '\n",
    "    neg_text = ' '\n",
    "    for val in all_tweets.text:\n",
    "        if val[:2] != 'RT':\n",
    "            all_text = all_text + ' ' + val\n",
    "    for val in positive.text:\n",
    "        if val[:2] != 'RT':\n",
    "            pos_text = pos_text + ' ' + val\n",
    "    for val in negative.text:\n",
    "        if val[:2] != 'RT':\n",
    "            neg_text = neg_text + ' ' + val\n",
    "    all_words = Text_Cleaner(all_text, tokens = True)\n",
    "    pos_words = Text_Cleaner(pos_text, tokens = True)\n",
    "    neg_words = Text_Cleaner(neg_text, tokens = True)\n",
    "    info = [company]\n",
    "    info.extend([len(all_words), len(pos_words), len(neg_words)])\n",
    "    info.extend([len(set(all_words)), len(set(pos_words)), len(set(neg_words))])\n",
    "    all_clean_words = [word for word in set(all_words) if word in english_vocab]\n",
    "    pos_clean_words = [word for word in set(pos_words) if word in english_vocab]\n",
    "    neg_clean_words = [word for word in set(neg_words) if word in english_vocab]\n",
    "    info.extend([len(all_clean_words), len(pos_clean_words), len(neg_clean_words)])\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting language composition for all companies\n",
    "all_comps = []\n",
    "for name in list(food.name.unique()):\n",
    "    all_comps.append(lang_composition(name))\n",
    "    \n",
    "lang_comp_df = pd.DataFrame(all_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#renaming all columns\n",
    "lang_comp_df.columns = ['name', 'all_words','pos_all_words', 'neg_all_words', 'unique_words', 'pos_unique_words', 'neg_unique_words', \n",
    "                        'english_words', 'pos_english_words', 'neg_english_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating percentages for analysis\n",
    "lang_comp_df['percent_unique'] = lang_comp_df['unique_words'] / lang_comp_df['all_words'] * 100\n",
    "lang_comp_df['percent_english'] = lang_comp_df['english_words'] / lang_comp_df['unique_words'] * 100\n",
    "lang_comp_df['pos_percent_unique'] = lang_comp_df['pos_unique_words'] / lang_comp_df['pos_all_words'] * 100\n",
    "lang_comp_df['pos_percent_english'] = lang_comp_df['pos_english_words'] / lang_comp_df['pos_unique_words'] * 100\n",
    "lang_comp_df['neg_percent_unique'] = lang_comp_df['neg_unique_words'] / lang_comp_df['neg_all_words'] * 100\n",
    "lang_comp_df['neg_percent_english'] = lang_comp_df['neg_english_words'] / lang_comp_df['neg_unique_words'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merging sentiments \n",
    "lang_comp_df = pd.merge(lang_comp_df, food.groupby('name', as_index=False)['sentiment_score'].mean(), \n",
    "       on='name')\n",
    "\n",
    "#Merging only negative sentiments \n",
    "food_neg = food.loc[food['sentiment_dummies'] == -1].groupby('name',as_index=False)['sentiment_score'].mean()\n",
    "food_neg.columns = ['name', 'neg_sentiment']\n",
    "lang_comp_df = pd.merge(lang_comp_df, food_neg, on='name')\n",
    "\n",
    "#Merging only positive sentiments\n",
    "food_pos = food.loc[food['sentiment_dummies'] == 1].groupby('name',as_index=False)['sentiment_score'].mean()\n",
    "food_neg.columns = ['name', 'pos_sentiment']\n",
    "lang_comp_df = pd.merge(lang_comp_df, food_pos, on='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_words             -0.453531\n",
       "pos_all_words         -0.344574\n",
       "neg_all_words         -0.553493\n",
       "unique_words          -0.280634\n",
       "pos_unique_words      -0.199107\n",
       "neg_unique_words      -0.434417\n",
       "english_words         -0.243947\n",
       "pos_english_words     -0.180855\n",
       "neg_english_words     -0.383851\n",
       "percent_unique         0.311866\n",
       "percent_english        0.298892\n",
       "pos_percent_unique     0.193846\n",
       "pos_percent_english    0.153271\n",
       "neg_percent_unique     0.532893\n",
       "neg_percent_english    0.581626\n",
       "sentiment_score_x      1.000000\n",
       "neg_sentiment          0.536657\n",
       "sentiment_score_y      0.512859\n",
       "Name: sentiment_score_x, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#strongest correlations with overall sentiment- percent unique (.31), percent English (0.29)\n",
    "lang_comp_df.corr()['sentiment_score_x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Looking at company tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_tweets = pd.read_csv('./company_tweets.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10920, 11)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_tweets['time_tweeted'] = pd.to_datetime(company_tweets['time_tweeted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#there are no duplicates\n",
    "company_tweets.unique_code.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping potbelly & redrobin\n",
    "company_tweets.drop(list(company_tweets.loc[(company_tweets['name'] == '@redrobinburgers')\n",
    "                  |(company_tweets['name'] == '@Potbelly')].index), axis=0,\n",
    "                   inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting normally spelled names\n",
    "mapper = {'@DennysDiner': 'Dennys', '@ChipotleTweets': 'Chipotle',\n",
    "         '@McDonalds': 'McDonalds', '@Wendys': 'Wendys', '@Starbucks':'Starbucks',\n",
    "         '@dunkindonuts':'Dunkin_Donuts', '@dominos': 'Dominos', '@shakeshack': 'Shake_Shack',\n",
    "         '@sonicdrivein': 'Sonic', '@wingstop': 'Wingstop', '@CrackerBarrel': 'Cracker_Barrel', \n",
    "         '@redrobinburgers': 'Red_Robin', '@Potbelly': 'Potbelly'}\n",
    "\n",
    "company_tweets['Company'] = company_tweets.name.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-03-30 09:48:49')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food.time_tweeted.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_tweets.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping all company tweets that occurred after consumer tweet collection stopped\n",
    "company_tweets.drop(list(\n",
    "    company_tweets.loc[company_tweets['time_tweeted'] > food.time_tweeted.max()].index), axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8080, 12)\n"
     ]
    }
   ],
   "source": [
    "print(company_tweets.shape)\n",
    "company_tweets = company_tweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping values that are before consumer observations\n",
    "company_tweets.drop(list(company_tweets.loc[company_tweets['time_tweeted'] < \n",
    "                                       food.time_tweeted.min()].index), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_tweets = company_tweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 240.76it/s]\n"
     ]
    }
   ],
   "source": [
    "#Getting the hashtags for each company\n",
    "hashtags = {}\n",
    "callnames = {}\n",
    "for company in tqdm(list(company_tweets.Company.unique())): \n",
    "    df = company_tweets.loc[company_tweets['Company'] == company]\n",
    "    hash_base = []\n",
    "    call_base = []\n",
    "    for val in df['text']:\n",
    "        hashes = re.findall(r'#[A-Za-z0-9]+\\b', val)\n",
    "        calls = re.findall(r'@[A-Za-z0-9]+\\b', val)\n",
    "        hash_base.extend(hashes)\n",
    "        call_base.extend(calls)\n",
    "    hashtags[company] = hash_base\n",
    "    callnames[company] = call_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slevin886/anaconda2/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'#[A-Za-z0-9]+\\b')\n",
    "#Only interested in customers independently using the hashtags\n",
    "hash_check = food.loc[food['is_a_retweet'] == 0]\n",
    "hash_check['text'] = hash_check.text.apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Identifying customer use of company hashtags\n",
    "\n",
    "company_counts = {}\n",
    "for company in list(company_tweets.Company.unique()):\n",
    "    hash_comp = hash_check.loc[hash_check['name'] == company]\n",
    "    hashes = {}\n",
    "    for val in hash_comp['text']:\n",
    "        for word in val:\n",
    "            if word in hashtags[company]:\n",
    "                try: \n",
    "                    hashes[word] += 1\n",
    "                except:\n",
    "                    hashes[word] = 1\n",
    "    company_counts[company] = hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#putting statistics on hashtags/use into new company profile dataframe\n",
    "new_df = pd.DataFrame(company_counts).T.sum(axis=1).reset_index()\n",
    "new_df.columns = ['name', 'customer_hash_use']\n",
    "new_df['customer_unique_hashes'] = pd.DataFrame(company_counts).T.count(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#examining company specific twitter behavior\n",
    "comp_hash_uses = []\n",
    "comp_unique_hashes = []\n",
    "comp_handle_uses = []\n",
    "comp_unique_handle_uses = []\n",
    "for company in new_df.name.tolist():\n",
    "    comp_hash_uses.append(len(hashtags[company]))\n",
    "    comp_unique_hashes.append(len(set(hashtags[company])))\n",
    "    comp_handle_uses.append(len(callnames[company]))\n",
    "    comp_unique_handle_uses.append(len(set(callnames[company])))\n",
    "\n",
    "new_df['comp_hash_uses'] = comp_hash_uses\n",
    "new_df['comp_unique_hashes'] = comp_unique_hashes\n",
    "new_df['comp_handle_uses'] = comp_handle_uses\n",
    "new_df['comp_unique_handle_uses'] = comp_unique_handle_uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grouping company tweet info\n",
    "comp_grouped = company_tweets.groupby('Company', as_index=False).mean().drop(\n",
    "                            ['is_a_retweet', 'is_quote_status', 'unique_code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_merged = pd.merge(comp_grouped, new_df, left_on='Company', right_on='name')\n",
    "comp_merged.drop('name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_merged['market_cap'] = merged_stock.groupby('name')['Market Cap'].mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_merged['sentiment'] = food.groupby('name')['sentiment_score'].mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Correlations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "comp_unique_handle_uses   -0.154493\n",
       "retweet_count             -0.072436\n",
       "number_of_tweets_total    -0.070891\n",
       "comp_hash_uses            -0.041042\n",
       "customer_unique_hashes     0.046609\n",
       "comp_unique_hashes         0.091145\n",
       "market_cap                 0.110416\n",
       "comp_handle_uses           0.251164\n",
       "customer_hash_use          0.303470\n",
       "favorite_count             0.311198\n",
       "company_followers_count    0.333409\n",
       "followers_count            0.333409\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Sentiment Correlations')\n",
    "comp_merged.corr()['sentiment'].sort_values()[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day of week optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ensuring that every weekday has equal representation (Friday will be slightly short-changed (by ~3 hours)\n",
    "day_week = food.loc[(food['time_tweeted'] > '2018-03-09')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting day nums to names, grouping by company and day of week and aggregating. \n",
    "mapper = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\n",
    "day_week = day_week.groupby(['name', 'weekday'], as_index=False).agg({'sentiment_score':'mean', 'user_name': 'count',\n",
    "                                                          'retweet_count':'mean', 'favorite_count':'mean'})\n",
    "\n",
    "day_week['weekday'] = day_week['weekday'].map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#including average sentiment for visualization\n",
    "merge = day_week.groupby('weekday', as_index=False)['sentiment_score'].mean()\n",
    "merge.columns = ['weekday', 'avg_sentiment']\n",
    "day_week = pd.merge(day_week, merge, on='weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets by day\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "weekday\n",
       "Sunday        4345\n",
       "Saturday      7259\n",
       "Monday        7549\n",
       "Wednesday     9024\n",
       "Friday        9840\n",
       "Thursday     12736\n",
       "Tuesday      13541\n",
       "Name: user_name, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('number of tweets by day')\n",
    "day_week.groupby('weekday')['user_name'].sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average sentiment by day\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "weekday\n",
       "Saturday     0.089578\n",
       "Sunday       0.100064\n",
       "Friday       0.116223\n",
       "Thursday     0.116878\n",
       "Monday       0.125332\n",
       "Wednesday    0.131134\n",
       "Tuesday      0.156336\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('average sentiment by day')\n",
    "day_week.groupby('weekday')['sentiment_score'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Beginning profile analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profiles = food.dropna(subset=['user_profile_text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54646, 21)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#only interested in unique profiles\n",
    "profiles = profiles.drop_duplicates(subset='user_name').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40482, 21)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping columns I don't need for this\n",
    "profiles.drop(['user_coordinates', 'unique_code', 'day_date', 'text_sentiment'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding an hour dummy\n",
    "profiles['hour'] = profiles.time_tweeted.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting rid of columns I don't need anymore\n",
    "profiles = profiles.drop(['time_tweeted', 'Company'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Maybe the number of hashtags a person uses is indicative of their personality/disposition towards the product\n",
    "def num_hashes(text):\n",
    "    \"\"\"Counts the number of hashtags in the profile\"\"\"\n",
    "    text = text.split()\n",
    "    num_hashes = 0\n",
    "    for word in text:\n",
    "        if word[0] == '#':\n",
    "            num_hashes = num_hashes + 1\n",
    "    return num_hashes\n",
    "\n",
    "profiles['number_of_hashes'] = profiles['user_profile_text'].apply(num_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08248110271231658"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8% of users use a hashtag in their profile\n",
    "np.sum(profiles.number_of_hashes > 1) / profiles.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding a dummy variable if the user lists a location, possibly indicator\n",
    "profiles['lists_location'] = profiles.user_location.isnull().astype(int)\n",
    "\n",
    "#unreliable for now to include\n",
    "profiles = profiles.drop('user_location', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaning_additional(text):\n",
    "    \"\"\"standardizing contractions, removing urls, removing retweet indicators, removing handles\"\"\"\n",
    "    text = re.sub(r'https[\\S]+', ' ', text)\n",
    "    text = re.sub(r'can\\'t', 'can not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'don\\'t', 'do not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'isn\\'t', 'is not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'aren\\'t', 'are not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'wasn\\'t', 'was not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'weren\\'t', 'were not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'haven\\'t', 'have not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(rt|RT)', ' ', text)\n",
    "    text = re.sub(r'@[\\S]+', ' ', text)\n",
    "    text = re.sub(r'[0-9]*[a-zA-Z]+[0-9]+[a-zA-Z]*[0-9]*[a-zA-Z]*', ' ', text)\n",
    "    return text\n",
    "\n",
    "profiles['cleaned'] = profiles.user_profile_text.apply(cleaning_additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punctuation_cleaner(text):\n",
    "    \"\"\"Removes punctuation and places spaces\"\"\"\n",
    "    text = re.sub(r'\\\\n', ' ', text)      \n",
    "    text = re.sub(r'[!|?|.|,|(|)|||[|]|/|\\\\|-]', ' ', text)\n",
    "    return text\n",
    "\n",
    "profiles['cleaned'] = profiles.cleaned.apply(punctuation_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_hashes(text):\n",
    "    \"\"\"Turns camel case hashtags into separate readable words\"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    split2 = []\n",
    "    for val in words:\n",
    "        if val[0] == '#':\n",
    "            words = ' '.join(re.findall('[A-Z][^A-Z]*', val))\n",
    "            split2.append(words)\n",
    "        else:\n",
    "            split2.append(val)\n",
    "    return ' '.join(split2)\n",
    "\n",
    "profiles['without_hashes'] = profiles.cleaned.apply(split_hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### T-test for mean difference between sentiment with/without retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=2.3715728048141727, pvalue=0.026886738879467794)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full = [0.065029, 0.094908, 0.097293, 0.101714, 0.106599, 0.108249, 0.113954, 0.121663, 0.147069, 0.153147, 0.162750, 0.173199, 0.184626]\n",
    "no_retweet = [0.089,0.132,0.072,0.0439,0.089,0.055887,0.124,0.089954,0.111304,0.134573,0.084565]\n",
    "\n",
    "stats.ttest_ind(full, no_retweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Looking at possible occupational/regional tendencies with a noun phrase function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words that aren't caught that don't add value to function below\n",
    "drop_words = ['love','don t', 'twitter', 'don', 'tweets', 'opinions', 'your',\n",
    "             'born', 'never']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Profile_Noun_Finder(text):\n",
    "    \"\"\"Takes text, eliminates non-alpha characters, and returns all nouns\"\"\"\n",
    "    text = re.sub(r'(https)[^\\s]+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z]+', ' ', text)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    lowered = [word.lower() for word in words]\n",
    "    words = [word for word in lowered if len(word) > 2]\n",
    "    words = [word for word in words if word not in drop_words]\n",
    "    new_text = ' '.join(words)\n",
    "    wiki = TextBlob(new_text)\n",
    "    return list(wiki.noun_phrases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
