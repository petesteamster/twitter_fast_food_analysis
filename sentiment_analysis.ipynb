{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis and Consumer Profiling\n",
    "\n",
    "This script includes cleaning, EDA, feature creation, and some preliminary analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib \n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food = pd.read_csv('./fastfood.csv', dtype=object, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152378, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping malformed data (invalid index)\n",
    "food.drop(list(food.loc[food['unique_code'] == 'Nobody should be too big to fail...'].index),\n",
    "          axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping duplicate entries\n",
    "food = food.drop_duplicates(subset='unique_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66504, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eliminated more than 50% of observations\n",
    "food.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping all rows with all null values\n",
    "food = food.drop(food[food.isnull().all(axis=1)].index[0], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropping all values that are not company related (only 3 observations)\n",
    "to_drop = []\n",
    "\n",
    "for row_num, val in enumerate(food['Company']):\n",
    "    if val[0] != '@':\n",
    "        to_drop.append(row_num)\n",
    "        \n",
    "food = food.drop(to_drop, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replacing strings with integers\n",
    "mapper = {'True': 1, 'False': 0}\n",
    "food['user_is_verified'] = food.user_is_verified.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filling nulls and converting data types\n",
    "food['retweet_count'] = food.retweet_count.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set to run Midnight and 5pm EST everyday, the times are in UTC, making EST\n",
    "food['time_tweeted'] = pd.to_datetime(food['time_tweeted']) - pd.Timedelta(hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food['favorite_count'] = food.favorite_count.astype(int)\n",
    "food['number_of_people_they_follow'] = food.number_of_people_they_follow.astype(int)\n",
    "food['number_of_user_tweets'] = food.number_of_user_tweets.astype(int)\n",
    "food['user_followers_count'] = food.user_followers_count.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a basic name category that isnt the handle\n",
    "\n",
    "mapper = {'@DennysDiner': 'Dennys', '@ChipotleTweets': 'Chipotle',\n",
    "         '@McDonalds': 'McDonalds', '@Wendys': 'Wendys', '@Starbucks':'Starbucks',\n",
    "         '@dunkindonuts':'Dunkin_Donuts', '@dominos': 'Dominos', '@shakeshack': 'Shake_Shack',\n",
    "         '@sonicdrivein': 'Sonic', '@wingstop': 'Wingstop', '@CrackerBarrel': 'Cracker_Barrel', \n",
    "         '@redrobinburgers': 'Red_Robin', '@Potbelly': 'Potbelly'}\n",
    "\n",
    "food['name'] = food.Company.map(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examined multiple methods of cleaning for sentiment data, only the final method runs now, but \n",
    "have kept my trial methods below (hashed out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shouldnt be removing stopwords before sentiment analysis:\n",
    "#http://www.lrec-conf.org/proceedings/lrec2014/pdf/292_Paper.pdf\n",
    "#testing different functions for preprocessing text for sentiment analysis\n",
    "\n",
    "\n",
    "def Text_Cleaner(text, tokens=False):\n",
    "    \"\"\"Takes text, eliminates URLS, replaces contractions, tokenizes, \n",
    "    removes company names, lower cases, removes calls to twitter handles, \n",
    "    returns a string, same as version 1, but only looking at words\"\"\"\n",
    "    text = re.sub(r'(https)[^\\s]+', '', text)\n",
    "    text = re.sub(r'can\\'t', 'can not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'don\\'t', 'do not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'isn\\'t', 'is not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'aren\\'t', 'are not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'wasn\\'t', 'was not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'weren\\'t', 'were not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'haven\\'t', 'have not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(rt|RT)', '', text)\n",
    "    text = re.sub(r'@[a-zA-Z0-9]+', '', text)\n",
    "    text = re.sub('#', '', text)\n",
    "    text = re.sub(r'(wtf)+\\b', 'what the fuck', text, flags=re.IGNORECASE)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    if tokens:\n",
    "        words = [word.lower() for word in words]\n",
    "        return words\n",
    "    return ' '.join(words)\n",
    "\n",
    "# def Text_Cleaner_version_1(text):\n",
    "#     \"\"\"Takes text, eliminates URLS, replaces contractions, tokenizes, \n",
    "#     removes company names, lower cases, removes calls to twitter handles, \n",
    "#     returns a string\"\"\"\n",
    "#     text = re.sub(r'(https)[^\\s]+', '', text)\n",
    "#     text = re.sub(r'\\b(rt|RT)', '', text)\n",
    "#     text = re.sub(r'@[a-zA-Z0-9]+', '', text)\n",
    "#     text = re.sub('#', '', text)\n",
    "#     return text\n",
    "\n",
    "#     LOOKING AT VARIOUS METHODS FOR PREPROCESSING FOR SENTIMENT ANALYSIS\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     words = tokenizer.tokenize(text)\n",
    "#     lower = [x.lower() for x in words]\n",
    "#     words = [word for word in words if word != 'rt']\n",
    "#     eliminator = [re.sub(r'(mcdon|dunki|denn|redro|sonic|starb|shakesh|domino|crackerb|chipot|wend)[a-z]+','',x)\n",
    "#                   for x in lower]\n",
    "#     return ' '.join(eliminator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Creating a test set of uncleaned data to check the value of the Text_Cleaner functions\n",
    "# food['text_sentiment_no_clean'] = food['text'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #converting text with version 1\n",
    "# food['text_sentiment_v1'] = food['text'].apply(Text_Cleaner_version_1)\n",
    "# #Calculating sentiment with TextBlob\n",
    "# food['sentiment_score_v1'] = food['text_sentiment_v1'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting text with version 2\n",
    "food['text_sentiment'] = food['text'].apply(Text_Cleaner)\n",
    "#Calculating sentiment with TextBlob\n",
    "food['sentiment_score'] = food['text_sentiment'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company\n",
       "@dominos            0.065029\n",
       "@dunkindonuts       0.094908\n",
       "@wingstop           0.097293\n",
       "@sonicdrivein       0.101714\n",
       "@McDonalds          0.106599\n",
       "@Wendys             0.108249\n",
       "@DennysDiner        0.113954\n",
       "@redrobinburgers    0.121663\n",
       "@ChipotleTweets     0.147069\n",
       "@CrackerBarrel      0.153147\n",
       "@Potbelly           0.162750\n",
       "@Starbucks          0.173199\n",
       "@shakeshack         0.184626\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at overall sentiment by company (version 1) (0 neutral, 1 positive, -1 negative)\n",
    "food.groupby('Company')['sentiment_score'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating sentiment dummy variables \n",
    "\n",
    "def dummy_maker(val):\n",
    "    \"\"\"Takes in a float and returns a dummy based on the value\n",
    "    to be used in pandas.apply\"\"\"\n",
    "    if val == 0:\n",
    "        return 0\n",
    "    elif val > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "food['sentiment_dummies'] = food['sentiment_score'].apply(dummy_maker)\n",
    "\n",
    "# food['sentiment_dummies_v2'] = food['sentiment_score_v2'].apply(dummy_maker)\n",
    "\n",
    "# food['sentiment_dummies_uncleaned'] = food['text_sentiment_no_clean'].apply(dummy_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    28738\n",
      " 0    27902\n",
      "-1     9860\n",
      "Name: sentiment_dummies, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print(food['sentiment_dummies_v1'].value_counts()) \n",
    "# print(food['sentiment_dummies_v2'].value_counts())\n",
    "print(food['sentiment_dummies'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i want choco butternut 😋 @dunkindonuts 🍩\n",
      "\n",
      "\n",
      "[1]\n",
      "RT @ChipotleTweets: Yes, guac is extra:\n",
      "E-nticing\n",
      "X-quisite\n",
      "T-empting\n",
      "R-eally yummy\n",
      "A-lways a good idea https://t.co/KDbNHCDtG8\n",
      "\n",
      "\n",
      "[0]\n",
      "There is nothing like @ChipotleTweets on a rainy day 👌\n",
      "\n",
      "\n",
      "[-1]\n",
      "@redrobinburgers Look, I am not sure who your target consumer *is but the vocal fry of your current ad makes your m… https://t.co/CosaqTbwtc\n",
      "\n",
      "\n",
      "[0]\n",
      "RT @DennysDiner: pepper is just emo salt\n",
      "\n",
      "\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#manually testing reliability of the different measures with a random subset\n",
    "random_numbers = list(np.random.randint(0, 62415, 5))\n",
    "for num, val in enumerate(food.iloc[random_numbers, :]['text']):\n",
    "    print(val)\n",
    "    print('\\n')\n",
    "    print(food.iloc[[num], :]['sentiment_dummies'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to quickly separate positive/negative tweets by company\n",
    "\n",
    "def negativity_formatter(company, hourly_rate=False):\n",
    "    \"\"\"This function accepts a company and returns either \n",
    "    the separate company, positive, negative dataframes (in that order)\n",
    "    OR the same order plus a dataframe of hourly rates, if hourly_rate = True\"\"\"\n",
    "    df = food.loc[food['name'] == company]\n",
    "    positive_df = df.loc[df['sentiment_dummies'] == 1]\n",
    "    negative_df = df.loc[df['sentiment_dummies'] == -1]\n",
    "    if not hourly_rate:\n",
    "        return df, positive_df, negative_df\n",
    "    else:\n",
    "        rate_df = pd.DataFrame()\n",
    "        rate_df['pos_count'] = positive_df.groupby(positive_df['time_tweeted'].dt.hour)['Company'].count()\n",
    "        rate_df['neg_count'] = negative_df.groupby(negative_df['time_tweeted'].dt.hour)['Company'].count()\n",
    "        rate_df['rate'] = rate_df['neg_count'] / (rate_df['pos_count'] + rate_df['neg_count'])\n",
    "        return rate_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43109774436090226"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#43% of tweets are retweets\n",
    "food.loc[food['text'].str[:2] == 'RT'].shape[0] / food.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a dummy for whether the tweet is a retweet or not\n",
    "retweets = []\n",
    "for val in food['text']:\n",
    "    if val[:2] == 'RT':\n",
    "        retweets.append(1)\n",
    "    else: \n",
    "        retweets.append(0)\n",
    "food['is_a_retweet'] = retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting rid of tweets without a user \n",
    "food = food.drop(list(food.loc[(food['user_name'].isnull())].index), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping regional affiliates\n",
    "associated_comps = []\n",
    "names = []\n",
    "for num, name in enumerate(food['user_name']):\n",
    "    test = re.findall(r'\\A(mcdon|dunki|redro|starbuc|shakesh|domino|crackerb)[a-z]+'\n",
    "                      , name, flags=re.IGNORECASE)\n",
    "    if test:\n",
    "        if name not in ['Dunkin Fails', 'Dunkin Kitti', \"McDonald's employee\"]:\n",
    "            associated_comps.append(num)\n",
    "            names.append(name)\n",
    "            \n",
    "food = food.drop(associated_comps, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    66441.000000\n",
       "mean         0.508662\n",
       "std          5.554575\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max        857.000000\n",
       "Name: favorite_count, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#favorite count is heavilly skewed, but keep for now\n",
    "food['favorite_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning to look at user profiles by company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#customer profiles still skewed by large means- looking at medians & means\n",
    "#creating a subset to examine the numeric characteristics of each company's customers\n",
    "individual_users = food.drop_duplicates(subset='user_name')\n",
    "\n",
    "customer_numeric_df = individual_users.groupby('name').agg({'favorite_count': ['mean', 'max'], \n",
    "                                      'number_of_people_they_follow': ['median', 'mean'],\n",
    "                                      'number_of_user_tweets': ['median', 'mean'],\n",
    "                                      'retweet_count': ['median', 'mean'],\n",
    "                                      'user_followers_count': ['median', 'mean'],\n",
    "                                       'user_is_verified': ['mean'],\n",
    "                                      'sentiment_score': ['mean'], \n",
    "                                      'is_a_retweet': 'mean',\n",
    "                                      'Company': 'count'})\n",
    "\n",
    "customer_numeric_df.columns = [' '.join(col).strip() for col in customer_numeric_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Not enough observations for Potbelly (684), RedRobin (1463), dropping\n",
    "food = food.drop(list(food.loc[(food['Company']== '@redrobinburgers')|\n",
    "                   (food['Company']== '@Potbelly')].index), axis=0)\n",
    "food = food.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MISSING VALUES FOR CERTAIN COMPANIES AT CERTAIN HOURS- hourly is flawed\n",
    "# negativity_by_comp = pd.DataFrame()\n",
    "# for val in food.name.unique().tolist():\n",
    "#     if val != 'McDonalds':\n",
    "#         values = list(negativity_formatter(val, hourly_rate=True)['rate'].values)\n",
    "#         negativity_by_comp[val] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food['day_date'] = food['time_tweeted'].dt.day\n",
    "food['weekday'] = food['time_tweeted'].dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning look at stock movements and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specifying trading days with sufficient information range\n",
    "stock_analysis = food.loc[(food['day_date'] > 11) & (food['day_date'] < 31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63463, 21)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_analysis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = pd.read_csv('./twitter_stocks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks.drop(['High', 'Low', 'Adj Close'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks['change'] = stocks['Close'] - stocks['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating datetime, and locating common trading dates\n",
    "stocks['Date'] = pd.to_datetime(stocks.Date)\n",
    "stocks['day'] = stocks['Date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks['day'] = stocks['Date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = stocks.loc[(stocks['day'] > 11) & (stocks['day'] < 31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making sure values match before merging\n",
    "stocks['Name'] = stocks.Name.str.replace('Shack Shack', 'Shake_Shack')\n",
    "stocks['Name'] = stocks.Name.str.replace('Cracker Barrel', 'Cracker_Barrel')\n",
    "stocks['Name'] = stocks.Name.str.replace('Dunkin Donuts', 'Dunkin_Donuts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grouping sentiments to merge\n",
    "stock_analysis = stock_analysis.groupby(['day_date', 'name'], as_index=False).agg(\n",
    "                                                    {'sentiment_score':'mean',\n",
    "                                                       'Company':'count'})\n",
    "\n",
    "stock_analysis.columns =  ['day_date', 'name', 'sentiment_score', 'num_observations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_stock = pd.merge(stock_analysis, stocks, left_on=['day_date', 'name'], right_on=['day', 'Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping redundant columns \n",
    "merged_stock.drop(['day', 'Name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_stock = merged_stock.sort_values(['name', 'day_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "day_date           -0.094365\n",
       "sentiment_score     1.000000\n",
       "num_observations   -0.013644\n",
       "Open               -0.014959\n",
       "Close              -0.012543\n",
       "Volume              0.145016\n",
       "Market Cap          0.089305\n",
       "change              0.102586\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#correlations between sentiment_score and various other indicators (volume/change/market_cap)\n",
    "merged_stock.corr()['sentiment_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Profiles of negative/positive sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set of english vocabulary\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lang_composition(company):\n",
    "    \"\"\"Takes a company name and returns the total number of words present, \n",
    "    the number of unique words present, and the number of those words that are in the English\n",
    "    language\"\"\"\n",
    "    all_tweets, positive, negative = negativity_formatter(company)\n",
    "    all_text = ' '\n",
    "    pos_text = ' '\n",
    "    neg_text = ' '\n",
    "    for val in all_tweets.text:\n",
    "        if val[:2] != 'RT':\n",
    "            all_text = all_text + ' ' + val\n",
    "    for val in positive.text:\n",
    "        if val[:2] != 'RT':\n",
    "            pos_text = pos_text + ' ' + val\n",
    "    for val in negative.text:\n",
    "        if val[:2] != 'RT':\n",
    "            neg_text = neg_text + ' ' + val\n",
    "    all_words = Text_Cleaner(all_text, tokens = True)\n",
    "    pos_words = Text_Cleaner(pos_text, tokens = True)\n",
    "    neg_words = Text_Cleaner(neg_text, tokens = True)\n",
    "    info = [company]\n",
    "    info.extend([len(all_words), len(pos_words), len(neg_words)])\n",
    "    info.extend([len(set(all_words)), len(set(pos_words)), len(set(neg_words))])\n",
    "    all_clean_words = [word for word in set(all_words) if word in english_vocab]\n",
    "    pos_clean_words = [word for word in set(pos_words) if word in english_vocab]\n",
    "    neg_clean_words = [word for word in set(neg_words) if word in english_vocab]\n",
    "    info.extend([len(all_clean_words), len(pos_clean_words), len(neg_clean_words)])\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting language composition for all companies\n",
    "all_comps = []\n",
    "for name in list(food.name.unique()):\n",
    "    all_comps.append(lang_composition(name))\n",
    "    \n",
    "lang_comp_df = pd.DataFrame(all_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#renaming all columns\n",
    "lang_comp_df.columns = ['name', 'all_words','pos_all_words', 'neg_all_words', 'unique_words', 'pos_unique_words', 'neg_unique_words', \n",
    "                        'english_words', 'pos_english_words', 'neg_english_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating percentages for analysis\n",
    "lang_comp_df['percent_unique'] = lang_comp_df['unique_words'] / lang_comp_df['all_words'] * 100\n",
    "lang_comp_df['percent_english'] = lang_comp_df['english_words'] / lang_comp_df['unique_words'] * 100\n",
    "lang_comp_df['pos_percent_unique'] = lang_comp_df['pos_unique_words'] / lang_comp_df['pos_all_words'] * 100\n",
    "lang_comp_df['pos_percent_english'] = lang_comp_df['pos_english_words'] / lang_comp_df['pos_unique_words'] * 100\n",
    "lang_comp_df['neg_percent_unique'] = lang_comp_df['neg_unique_words'] / lang_comp_df['neg_all_words'] * 100\n",
    "lang_comp_df['neg_percent_english'] = lang_comp_df['neg_english_words'] / lang_comp_df['neg_unique_words'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merging sentiments \n",
    "lang_comp_df = pd.merge(lang_comp_df, food.groupby('name', as_index=False)['sentiment_score'].mean(), \n",
    "       on='name')\n",
    "\n",
    "#Merging only negative sentiments \n",
    "food_neg = food.loc[food['sentiment_dummies'] == -1].groupby('name',as_index=False)['sentiment_score'].mean()\n",
    "food_neg.columns = ['name', 'neg_sentiment']\n",
    "lang_comp_df = pd.merge(lang_comp_df, food_neg, on='name')\n",
    "\n",
    "#Merging only positive sentiments\n",
    "food_pos = food.loc[food['sentiment_dummies'] == 1].groupby('name',as_index=False)['sentiment_score'].mean()\n",
    "food_neg.columns = ['name', 'pos_sentiment']\n",
    "lang_comp_df = pd.merge(lang_comp_df, food_pos, on='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_words             -0.453531\n",
       "pos_all_words         -0.344574\n",
       "neg_all_words         -0.553493\n",
       "unique_words          -0.280634\n",
       "pos_unique_words      -0.199107\n",
       "neg_unique_words      -0.434417\n",
       "english_words         -0.243947\n",
       "pos_english_words     -0.180855\n",
       "neg_english_words     -0.383851\n",
       "percent_unique         0.311866\n",
       "percent_english        0.298892\n",
       "pos_percent_unique     0.193846\n",
       "pos_percent_english    0.153271\n",
       "neg_percent_unique     0.532893\n",
       "neg_percent_english    0.581626\n",
       "sentiment_score_x      1.000000\n",
       "neg_sentiment          0.536657\n",
       "sentiment_score_y      0.512859\n",
       "Name: sentiment_score_x, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#strongest correlations with overall sentiment- percent unique (.31), percent English (0.29)\n",
    "lang_comp_df.corr()['sentiment_score_x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Looking at company tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_tweets = pd.read_csv('./company_tweets.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10920, 11)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_tweets['time_tweeted'] = pd.to_datetime(company_tweets['time_tweeted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#there are no duplicates\n",
    "company_tweets.unique_code.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping potbelly & redrobin\n",
    "company_tweets.drop(list(company_tweets.loc[(company_tweets['name'] == '@redrobinburgers')\n",
    "                  |(company_tweets['name'] == '@Potbelly')].index), axis=0,\n",
    "                   inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting normally spelled names\n",
    "mapper = {'@DennysDiner': 'Dennys', '@ChipotleTweets': 'Chipotle',\n",
    "         '@McDonalds': 'McDonalds', '@Wendys': 'Wendys', '@Starbucks':'Starbucks',\n",
    "         '@dunkindonuts':'Dunkin_Donuts', '@dominos': 'Dominos', '@shakeshack': 'Shake_Shack',\n",
    "         '@sonicdrivein': 'Sonic', '@wingstop': 'Wingstop', '@CrackerBarrel': 'Cracker_Barrel', \n",
    "         '@redrobinburgers': 'Red_Robin', '@Potbelly': 'Potbelly'}\n",
    "\n",
    "company_tweets['Company'] = company_tweets.name.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-03-30 09:48:49')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food.time_tweeted.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_tweets.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping all company tweets that occurred after consumer tweet collection stopped\n",
    "company_tweets.drop(list(\n",
    "    company_tweets.loc[company_tweets['time_tweeted'] > food.time_tweeted.max()].index), axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8080, 12)\n"
     ]
    }
   ],
   "source": [
    "print(company_tweets.shape)\n",
    "company_tweets = company_tweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping values that are before consumer observations\n",
    "company_tweets.drop(list(company_tweets.loc[company_tweets['time_tweeted'] < \n",
    "                                       food.time_tweeted.min()].index), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_tweets = company_tweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 157.47it/s]\n"
     ]
    }
   ],
   "source": [
    "#Getting the hashtags for each company\n",
    "hashtags = {}\n",
    "callnames = {}\n",
    "for company in tqdm(list(company_tweets.Company.unique())): \n",
    "    df = company_tweets.loc[company_tweets['Company'] == company]\n",
    "    hash_base = []\n",
    "    call_base = []\n",
    "    for val in df['text']:\n",
    "        hashes = re.findall(r'#[A-Za-z0-9]+\\b', val)\n",
    "        calls = re.findall(r'@[A-Za-z0-9]+\\b', val)\n",
    "        hash_base.extend(hashes)\n",
    "        call_base.extend(calls)\n",
    "    hashtags[company] = hash_base\n",
    "    callnames[company] = call_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slevin886/anaconda2/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'#[A-Za-z0-9]+\\b')\n",
    "#Only interested in customers independently using the hashtags\n",
    "hash_check = food.loc[food['is_a_retweet'] == 0]\n",
    "hash_check['text'] = hash_check.text.apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Identifying customer use of company hashtags\n",
    "\n",
    "company_counts = {}\n",
    "for company in list(company_tweets.Company.unique()):\n",
    "    hash_comp = hash_check.loc[hash_check['name'] == company]\n",
    "    hashes = {}\n",
    "    for val in hash_comp['text']:\n",
    "        for word in val:\n",
    "            if word in hashtags[company]:\n",
    "                try: \n",
    "                    hashes[word] += 1\n",
    "                except:\n",
    "                    hashes[word] = 1\n",
    "    company_counts[company] = hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#putting statistics on hashtags/use into new company profile dataframe\n",
    "new_df = pd.DataFrame(company_counts).T.sum(axis=1).reset_index()\n",
    "new_df.columns = ['name', 'customer_hash_use']\n",
    "new_df['customer_unique_hashes'] = pd.DataFrame(company_counts).T.count(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#examining company specific twitter behavior\n",
    "comp_hash_uses = []\n",
    "comp_unique_hashes = []\n",
    "comp_handle_uses = []\n",
    "comp_unique_handle_uses = []\n",
    "for company in new_df.name.tolist():\n",
    "    comp_hash_uses.append(len(hashtags[company]))\n",
    "    comp_unique_hashes.append(len(set(hashtags[company])))\n",
    "    comp_handle_uses.append(len(callnames[company]))\n",
    "    comp_unique_handle_uses.append(len(set(callnames[company])))\n",
    "\n",
    "new_df['comp_hash_uses'] = comp_hash_uses\n",
    "new_df['comp_unique_hashes'] = comp_unique_hashes\n",
    "new_df['comp_handle_uses'] = comp_handle_uses\n",
    "new_df['comp_unique_handle_uses'] = comp_unique_handle_uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grouping company tweet info\n",
    "comp_grouped = company_tweets.groupby('Company', as_index=False).mean().drop(\n",
    "                            ['is_a_retweet', 'is_quote_status', 'unique_code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_merged = pd.merge(comp_grouped, new_df, left_on='Company', right_on='name')\n",
    "comp_merged.drop('name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_merged['market_cap'] = merged_stock.groupby('name')['Market Cap'].mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_merged['sentiment'] = food.groupby('name')['sentiment_score'].mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Correlations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "comp_unique_handle_uses   -0.154493\n",
       "retweet_count             -0.072436\n",
       "number_of_tweets_total    -0.070891\n",
       "comp_hash_uses            -0.041042\n",
       "customer_unique_hashes     0.046609\n",
       "comp_unique_hashes         0.091145\n",
       "market_cap                 0.110416\n",
       "comp_handle_uses           0.251164\n",
       "customer_hash_use          0.303470\n",
       "favorite_count             0.311198\n",
       "company_followers_count    0.333409\n",
       "followers_count            0.333409\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Sentiment Correlations')\n",
    "comp_merged.corr()['sentiment'].sort_values()[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day of week optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensuring that every weekday has equal representation (Friday will be slightly short-changed (by ~3 hours)\n",
    "day_week = food.loc[(food['time_tweeted'] > '2018-03-09')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting day nums to names, grouping by company and day of week and aggregating. \n",
    "mapper = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\n",
    "day_week = day_week.groupby(['name', 'weekday'], as_index=False).agg({'sentiment_score':'mean', 'user_name': 'count',\n",
    "                                                          'retweet_count':'mean', 'favorite_count':'mean'})\n",
    "\n",
    "day_week['weekday'] = day_week['weekday'].map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#including average sentiment for visualization\n",
    "merge = day_week.groupby('weekday', as_index=False)['sentiment_score'].mean()\n",
    "merge.columns = ['weekday', 'avg_sentiment']\n",
    "day_week = pd.merge(day_week, merge, on='weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets by day\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "weekday\n",
       "Sunday        4345\n",
       "Saturday      7259\n",
       "Monday        7549\n",
       "Wednesday     9024\n",
       "Friday        9840\n",
       "Thursday     12736\n",
       "Tuesday      13541\n",
       "Name: user_name, dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('number of tweets by day')\n",
    "day_week.groupby('weekday')['user_name'].sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average sentiment by day\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "weekday\n",
       "Saturday     0.089578\n",
       "Sunday       0.100064\n",
       "Friday       0.116223\n",
       "Thursday     0.116878\n",
       "Monday       0.125332\n",
       "Wednesday    0.131134\n",
       "Tuesday      0.156336\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('average sentiment by day')\n",
    "day_week.groupby('weekday')['sentiment_score'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Beginning profile analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = food.dropna(subset=['user_profile_text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54646, 21)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#only interested in unique profiles\n",
    "profiles = profiles.drop_duplicates(subset='user_name').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40482, 21)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns I don't need for this\n",
    "profiles.drop(['user_coordinates', 'unique_code', 'day_date', 'text_sentiment'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding an hour dummy\n",
    "profiles['hour'] = profiles.time_tweeted.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of columns I don't need anymore\n",
    "profiles = profiles.drop(['time_tweeted', 'Company'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe the number of hashtags a person uses is indicative of their personality/disposition towards the product\n",
    "def num_hashes(text):\n",
    "    \"\"\"Counts the number of hashtags in the profile\"\"\"\n",
    "    text = text.split()\n",
    "    num_hashes = 0\n",
    "    for word in text:\n",
    "        if word[0] == '#':\n",
    "            num_hashes = num_hashes + 1\n",
    "    return num_hashes\n",
    "\n",
    "profiles['number_of_hashes'] = profiles['user_profile_text'].apply(num_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08248110271231658"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8% of users use a hashtag in their profile\n",
    "np.sum(profiles.number_of_hashes > 1) / profiles.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a dummy variable if the user lists a location, possibly indicator\n",
    "profiles['lists_location'] = profiles.user_location.isnull().astype(int)\n",
    "\n",
    "#unreliable for now to include\n",
    "profiles = profiles.drop('user_location', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_additional(text):\n",
    "    \"\"\"standardizing contractions, removing urls, removing retweet indicators, removing handles\"\"\"\n",
    "    text = re.sub(r'https[\\S]+', ' ', text)\n",
    "    text = re.sub(r'can\\'t', 'can not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'don\\'t', 'do not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'isn\\'t', 'is not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'aren\\'t', 'are not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'wasn\\'t', 'was not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'weren\\'t', 'were not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'haven\\'t', 'have not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(rt|RT)', ' ', text)\n",
    "    text = re.sub(r'@[\\S]+', ' ', text)\n",
    "    text = re.sub(r'[0-9]*[a-zA-Z]+[0-9]+[a-zA-Z]*[0-9]*[a-zA-Z]*', ' ', text)\n",
    "    return text\n",
    "\n",
    "profiles['cleaned'] = profiles.user_profile_text.apply(cleaning_additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_cleaner(text):\n",
    "    \"\"\"Removes punctuation and places spaces\"\"\"\n",
    "    text = re.sub(r'\\\\n', ' ', text)      \n",
    "    text = re.sub(r'[!|?|.|,|(|)|||[|]|/|\\\\|-]', ' ', text)\n",
    "    return text\n",
    "\n",
    "profiles['cleaned'] = profiles.cleaned.apply(punctuation_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashes(text):\n",
    "    \"\"\"Turns camel case hashtags into separate readable words\"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    split2 = []\n",
    "    for val in words:\n",
    "        if val[0] == '#':\n",
    "            words = ' '.join(re.findall('[A-Z][^A-Z]*', val))\n",
    "            split2.append(words)\n",
    "        else:\n",
    "            split2.append(val)\n",
    "    return ' '.join(split2)\n",
    "\n",
    "profiles['without_hashes'] = profiles.cleaned.apply(split_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40452    22 Gym Rat EliteGrinders_ pub stomper in ranke...\n",
       "40453                           what is happiness POG Life\n",
       "40454    The Remy part of the Mason Remy Alabama Show M...\n",
       "40455    For we are his workmanship created in Christ J...\n",
       "40456    Unapologetically honest I do not care if you l...\n",
       "40457    DB Coach American Heritage DB Coach SouthFlori...\n",
       "40458                                      Rest Easy Momma\n",
       "40459    A body and perpetual optimist Inform inspire e...\n",
       "40460                               Gotta Sparkle to Shine\n",
       "40461    Hip modern vegan rabbi living and working in N...\n",
       "40462    retired civil servant politics coach youth bas...\n",
       "40463                                                 AVHS\n",
       "40464    HTTR ItTakesEverything TarHeelNation Only The ...\n",
       "40465                                      Grafton 20 Kate\n",
       "40466    Foodie MtG Player who loves Counting to 10 or ...\n",
       "40467    Just a person trying to figure this whole thin...\n",
       "40468    love to read act like a child most days intere...\n",
       "40469                         Enjoying life to the fullest\n",
       "40470                the adventure is out there timetoplay\n",
       "40471    Mysterious Genuine BadAss Open Minded Descript...\n",
       "40472    Live everyday like it s your last nothing is p...\n",
       "40473            just a kid living life one step at a time\n",
       "40474    Director of Special Projects at Tax Foundation...\n",
       "40475    Fiscal and Economic Policy Leader Public Speak...\n",
       "40476    Fisherman by birth yachtsman car junkie studen...\n",
       "40477                          sports lover Louisville fan\n",
       "40478    Proud to be gay proud to be Jewish proud to be...\n",
       "40479    Drive a bus school district in Palm Beach Coun...\n",
       "40480    morning traffic reporter Lover of stout and sc...\n",
       "40481    LowLife Secure The Bag Cap Hooper Trust The Pr...\n",
       "Name: without_hashes, dtype: object"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words that aren't caught that don't add value to function below\n",
    "drop_words = ['love','don t', 'twitter', 'don', 'tweets', 'opinions', 'your',\n",
    "             'born', 'never']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Profile_Noun_Finder(text):\n",
    "    \"\"\"Takes text, eliminates non-alpha characters, and returns all nouns\"\"\"\n",
    "    text = re.sub(r'(https)[^\\s]+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z]+', ' ', text)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    lowered = [word.lower() for word in words]\n",
    "    words = [word for word in lowered if len(word) > 2]\n",
    "    words = [word for word in words if word not in drop_words]\n",
    "    new_text = ' '.join(words)\n",
    "    wiki = TextBlob(new_text)\n",
    "    return list(wiki.noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting rid of empty profiles\n",
    "without_retweets = without_retweets.dropna(subset=['user_profile_text'])\n",
    "\n",
    "#running function on profiles\n",
    "without_retweets['nouns'] = without_retweets['user_profile_text'].apply(Profile_Noun_Finder)\n",
    "\n",
    "#subsetting for positive reviews\n",
    "positive_profiles = without_retweets.loc[without_retweets['sentiment_dummies'] == 1]\n",
    "\n",
    "#flattening the list\n",
    "flattened_nouns = [word for parts in positive_profiles['nouns'] for word in parts]\n",
    "\n",
    "#looking at the 50 most common noun phrases\n",
    "Counter(flattened_nouns).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating word counts based on np_counts feature of text_blob\n",
    "texters = ' '.join(positive_profiles['user_profile_text'])\n",
    "dic_values = TextBlob(texters).np_counts\n",
    "favorible_words = pd.DataFrame()\n",
    "favorible_words['words'] = dic_values.keys()\n",
    "favorible_words['count'] = dic_values.values()\n",
    "favorible_words.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('youtube channel', 7),\n",
       " ('social media', 4),\n",
       " ('new youtube', 3),\n",
       " ('personal account', 3),\n",
       " ('full time', 3),\n",
       " ('free time', 3),\n",
       " ('wife mom', 3),\n",
       " ('mhs tfa', 3),\n",
       " ('video games', 2),\n",
       " ('misanthropic attention whore talk', 2),\n",
       " ('music vinylpizza podcast writer', 2),\n",
       " ('link bio', 2),\n",
       " ('black man', 2),\n",
       " ('new things life', 2),\n",
       " ('chance waste', 2),\n",
       " ('everyday life', 2),\n",
       " ('proud mother', 2),\n",
       " ('parody account', 2),\n",
       " ('new products', 2),\n",
       " ('nintendo switch content', 2),\n",
       " ('animal lover', 2),\n",
       " ('gmail com', 2),\n",
       " ('proud father', 2),\n",
       " ('cool things', 2),\n",
       " ('news junkie animal lover', 2),\n",
       " ('culture addict theater', 2),\n",
       " ('news senior producer newhousesu grad retweets endorsements', 2),\n",
       " ('san diego', 2),\n",
       " ('stupid people', 2),\n",
       " ('union news press blogs jobs', 2),\n",
       " ('simple girl', 2),\n",
       " ('gon na', 2),\n",
       " ('social justice', 2),\n",
       " ('husband father', 2),\n",
       " ('wonderful children', 2),\n",
       " ('irony creator good music', 2),\n",
       " ('bad music keeper', 2),\n",
       " ('secret potato burrito sauce', 2),\n",
       " ('producer bigtennetwork producer btntailgate hcbeavers emuswimdive wotv detroit espn alum',\n",
       "  2),\n",
       " ('national sports emmy winner', 2),\n",
       " ('air force', 2),\n",
       " ('beautiful laugh ass', 2),\n",
       " ('happy people', 2),\n",
       " ('black sheep', 2),\n",
       " ('comeback instagram sarahcoolbeams', 2),\n",
       " ('pursuit happiness', 2),\n",
       " ('black lives matter', 2),\n",
       " ('old manager lyriqyank loyaltyank yank dbtb', 2),\n",
       " ('good things', 2),\n",
       " ('army brat', 2)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subsetting for positive reviews\n",
    "negative_profiles = without_retweets.loc[without_retweets['sentiment_dummies'] == -1]\n",
    "\n",
    "#flattening the list\n",
    "flattened_nouns = [word for parts in negative_profiles['nouns'] for word in parts]\n",
    "\n",
    "#looking at the 50 most common noun phrases\n",
    "Counter(flattened_nouns).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>love</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>twitter</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>ig</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>wife</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>god</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>instagram</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>husband</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>’ s</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>father</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>snapchat</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tweets</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>youtube</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>’ m</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>fan</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>proud</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>jesus</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>| #</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>coffee</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>founder</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>dad</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>youtuber</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>gamer</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>never</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>opinions</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>dj</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>christ</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>views</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>maga</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>photographer</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>midway</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>course 🤗</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2304</th>\n",
       "      <td>twitter parties</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>passive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>horror freak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>genuine soul</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>👽graphic design</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>sip wine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>emoji</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>rupaul</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>olb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>personally blocked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>herne</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339</th>\n",
       "      <td>canterbury</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>🔥🏈🔥 news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>vsualumni</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>blackswarm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2334</th>\n",
       "      <td>state university #</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>valdosta</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>last..tough people</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>aficionado 👾</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>tough</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>teamfume keep god</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>fumeresurge</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>teamfume.com overall</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326</th>\n",
       "      <td>mouthana</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>inquires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>mashable + @ nytimes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2322</th>\n",
       "      <td>stuff @</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6199</th>\n",
       "      <td>life i</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     words  count\n",
       "29                    love     37\n",
       "149                twitter     27\n",
       "167                     ig     25\n",
       "572                   wife     24\n",
       "176                    god     23\n",
       "772              instagram     21\n",
       "538                husband     21\n",
       "828                    ’ s     20\n",
       "484                 father     19\n",
       "397               snapchat     18\n",
       "16                  tweets     17\n",
       "93                 youtube     17\n",
       "1268                   ’ m     17\n",
       "200                    fan     15\n",
       "294                  proud     14\n",
       "549                  jesus     14\n",
       "842                    | #     13\n",
       "553                 coffee     12\n",
       "17                 founder     12\n",
       "83                     dad     12\n",
       "810               youtuber     11\n",
       "1530                 gamer     11\n",
       "436                  never     10\n",
       "211               opinions     10\n",
       "752           entrepreneur     10\n",
       "2158                    dj     10\n",
       "837                 christ      9\n",
       "682                  views      9\n",
       "269                   maga      9\n",
       "734           photographer      9\n",
       "...                    ...    ...\n",
       "2306                midway      1\n",
       "2305              course 🤗      1\n",
       "2304       twitter parties      1\n",
       "2302               passive      1\n",
       "2301          horror freak      1\n",
       "2300          genuine soul      1\n",
       "2299       👽graphic design      1\n",
       "2318              sip wine      1\n",
       "2320                 emoji      1\n",
       "2343                rupaul      1\n",
       "2332                   olb      1\n",
       "2342    personally blocked      1\n",
       "2340                 herne      1\n",
       "2339            canterbury      1\n",
       "2337              🔥🏈🔥 news      1\n",
       "2336             vsualumni      1\n",
       "2335            blackswarm      1\n",
       "2334    state university #      1\n",
       "2333              valdosta      1\n",
       "2331    last..tough people      1\n",
       "2321          aficionado 👾      1\n",
       "2330                 tough      1\n",
       "2329     teamfume keep god      1\n",
       "2328           fumeresurge      1\n",
       "2327  teamfume.com overall      1\n",
       "2326              mouthana      1\n",
       "2325              inquires      1\n",
       "2323  mashable + @ nytimes      1\n",
       "2322               stuff @      1\n",
       "6199                life i      1\n",
       "\n",
       "[6200 rows x 2 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating word counts based on np_counts feature of text_blob\n",
    "texters = ' '.join(negative_profiles['user_profile_text'])\n",
    "dic_values = TextBlob(texters).np_counts\n",
    "favorible_words = pd.DataFrame()\n",
    "favorible_words['words'] = dic_values.keys()\n",
    "favorible_words['count'] = dic_values.values()\n",
    "favorible_words.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dunkin vs. Starbucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slevin886/anaconda2/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dunkin = food.loc[food['Company'] == '@dunkindonuts'] \n",
    "dunkin['hour'] = dunkin['time_tweeted'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hour\n",
       "0.0     0.055933\n",
       "1.0     0.082589\n",
       "2.0     0.165090\n",
       "3.0     0.155026\n",
       "4.0     0.083048\n",
       "5.0     0.168492\n",
       "6.0     0.096741\n",
       "7.0     0.130059\n",
       "8.0     0.068469\n",
       "9.0     0.074440\n",
       "10.0    0.066149\n",
       "11.0    0.080584\n",
       "12.0    0.062593\n",
       "13.0    0.083717\n",
       "14.0    0.072379\n",
       "15.0    0.093971\n",
       "16.0    0.116218\n",
       "17.0    0.124282\n",
       "18.0    0.110968\n",
       "19.0    0.072887\n",
       "20.0    0.037402\n",
       "21.0    0.128900\n",
       "22.0    0.096105\n",
       "23.0    0.081113\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunkin.groupby('hour')['sentiment_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_dunkin = dunkin.loc[dunkin['sentiment_dummies'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slevin886/anaconda2/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dunkin['user_profile_text'] = dunkin['user_profile_text'].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slevin886/anaconda2/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dunkin['nouns'] = dunkin['user_profile_text'].apply(Profile_Noun_Finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_dunkin = dunkin.loc[dunkin['sentiment_dummies'] == 1]\n",
    "negative_dunkin = dunkin.loc[dunkin['sentiment_dummies'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nouns = []\n",
    "for val in positive_dunkin['nouns']:\n",
    "    if val:\n",
    "        words = val[0]\n",
    "        words = words.split()\n",
    "        for word in words:\n",
    "            nouns.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_positive_dunk_words = Counter(nouns).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nouns = []\n",
    "for val in negative_dunkin['nouns']:\n",
    "    if val:\n",
    "        words = val[0]\n",
    "        words = words.split()\n",
    "        for word in words:\n",
    "            nouns.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_dunk_words = Counter(nouns).most_common(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
