{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis and Consumer Profiling\n",
    "\n",
    "This script includes cleaning, EDA, feature creation, and some preliminary analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib \n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food = pd.read_csv('./fastfood.csv', dtype=object, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418332, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping malformed data (invalid index)\n",
    "food.drop(list(food.loc[food['unique_code'] == 'Nobody should be too big to fail...'].index),\n",
    "          axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping duplicate entries\n",
    "food = food.drop_duplicates(subset='unique_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143983, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eliminated more than 50% of observations\n",
    "food.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Company', 'favorite_count', 'number_of_people_they_follow',\n",
       "       'number_of_user_tweets', 'retweet_count', 'text', 'time_tweeted',\n",
       "       'unique_code', 'user_coordinates', 'user_followers_count',\n",
       "       'user_is_verified', 'user_location', 'user_name', 'user_profile_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping all rows with all null values\n",
    "food = food.drop(food[food.isnull().all(axis=1)].index[0], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropping all values that are not company related (only 3 observations)\n",
    "to_drop = []\n",
    "\n",
    "for row_num, val in enumerate(food['Company']):\n",
    "    if val[0] != '@':\n",
    "        to_drop.append(row_num)\n",
    "        \n",
    "food = food.drop(to_drop, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replacing strings with integers\n",
    "mapper = {'True': 1, 'False': 0}\n",
    "food['user_is_verified'] = food.user_is_verified.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filling nulls and converting data types\n",
    "food['retweet_count'] = food.retweet_count.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set to run Midnight and 5pm EST everyday, the times are in UTC, making EST\n",
    "food['time_tweeted'] = pd.to_datetime(food['time_tweeted']) - pd.Timedelta(hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final time range before terminating EC2 2018-03-03 13:56:18 2018-04-19 16:48:37\n"
     ]
    }
   ],
   "source": [
    "print('Final time range before terminating EC2', food['time_tweeted'].min(), food['time_tweeted'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food['favorite_count'] = food.favorite_count.astype(int)\n",
    "food['number_of_people_they_follow'] = food.number_of_people_they_follow.astype(int)\n",
    "food['number_of_user_tweets'] = food.number_of_user_tweets.astype(int)\n",
    "food['user_followers_count'] = food.user_followers_count.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a basic name category that isnt the handle\n",
    "\n",
    "mapper = {'@DennysDiner': 'Dennys', '@ChipotleTweets': 'Chipotle',\n",
    "         '@McDonalds': 'McDonalds', '@Wendys': 'Wendys', '@Starbucks':'Starbucks',\n",
    "         '@dunkindonuts':'Dunkin_Donuts', '@dominos': 'Dominos', '@shakeshack': 'Shake_Shack',\n",
    "         '@sonicdrivein': 'Sonic', '@wingstop': 'Wingstop', '@CrackerBarrel': 'Cracker_Barrel', \n",
    "         '@redrobinburgers': 'Red_Robin', '@Potbelly': 'Potbelly'}\n",
    "\n",
    "food['name'] = food.Company.map(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examined multiple methods of cleaning for sentiment data, only the final method runs now, but \n",
    "have kept my trial methods below (hashed out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shouldnt be removing stopwords before sentiment analysis:\n",
    "#http://www.lrec-conf.org/proceedings/lrec2014/pdf/292_Paper.pdf\n",
    "#testing different functions for preprocessing text for sentiment analysis\n",
    "\n",
    "\n",
    "def Text_Cleaner(text, tokens=False):\n",
    "    \"\"\"Takes text, eliminates URLS, replaces contractions, tokenizes, \n",
    "    removes company names, lower cases, removes calls to twitter handles, \n",
    "    returns a string, same as version 1, but only looking at words\"\"\"\n",
    "    text = re.sub(r'(https)[^\\s]+', '', text)\n",
    "    text = re.sub(r'can\\'t', 'can not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'don\\'t', 'do not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'isn\\'t', 'is not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'aren\\'t', 'are not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'wasn\\'t', 'was not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'weren\\'t', 'were not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'haven\\'t', 'have not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(rt|RT)', '', text)\n",
    "    text = re.sub(r'@[a-zA-Z0-9]+', '', text)\n",
    "    text = re.sub('#', '', text)\n",
    "    text = re.sub(r'(wtf)+\\b', 'what the fuck', text, flags=re.IGNORECASE)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    if tokens:\n",
    "        words = [word.lower() for word in words]\n",
    "        return words\n",
    "    return ' '.join(words)\n",
    "\n",
    "# def Text_Cleaner_version_1(text):\n",
    "#     \"\"\"Takes text, eliminates URLS, replaces contractions, tokenizes, \n",
    "#     removes company names, lower cases, removes calls to twitter handles, \n",
    "#     returns a string\"\"\"\n",
    "#     text = re.sub(r'(https)[^\\s]+', '', text)\n",
    "#     text = re.sub(r'\\b(rt|RT)', '', text)\n",
    "#     text = re.sub(r'@[a-zA-Z0-9]+', '', text)\n",
    "#     text = re.sub('#', '', text)\n",
    "#     return text\n",
    "\n",
    "#     LOOKING AT VARIOUS METHODS FOR PREPROCESSING FOR SENTIMENT ANALYSIS\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     words = tokenizer.tokenize(text)\n",
    "#     lower = [x.lower() for x in words]\n",
    "#     words = [word for word in words if word != 'rt']\n",
    "#     eliminator = [re.sub(r'(mcdon|dunki|denn|redro|sonic|starb|shakesh|domino|crackerb|chipot|wend)[a-z]+','',x)\n",
    "#                   for x in lower]\n",
    "#     return ' '.join(eliminator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Creating a test set of uncleaned data to check the value of the Text_Cleaner functions\n",
    "# food['text_sentiment_no_clean'] = food['text'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #converting text with version 1\n",
    "# food['text_sentiment_v1'] = food['text'].apply(Text_Cleaner_version_1)\n",
    "# #Calculating sentiment with TextBlob\n",
    "# food['sentiment_score_v1'] = food['text_sentiment_v1'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting text with version 2\n",
    "food['text_sentiment'] = food['text'].apply(Text_Cleaner)\n",
    "#Calculating sentiment with TextBlob\n",
    "food['sentiment_score'] = food['text_sentiment'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company\n",
       "@dominos            0.054447\n",
       "@McDonalds          0.088885\n",
       "@DennysDiner        0.089561\n",
       "@dunkindonuts       0.093069\n",
       "@Wendys             0.106140\n",
       "@redrobinburgers    0.114117\n",
       "@wingstop           0.115563\n",
       "@Starbucks          0.120630\n",
       "@sonicdrivein       0.122653\n",
       "@Potbelly           0.141328\n",
       "@ChipotleTweets     0.143646\n",
       "@CrackerBarrel      0.152685\n",
       "@shakeshack         0.169174\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at overall sentiment by company (version 1) (0 neutral, 1 positive, -1 negative)\n",
    "food.groupby('Company')['sentiment_score'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating sentiment dummy variables \n",
    "\n",
    "def dummy_maker(val):\n",
    "    \"\"\"Takes in a float and returns a dummy based on the value\n",
    "    to be used in pandas.apply\"\"\"\n",
    "    if val == 0:\n",
    "        return 0\n",
    "    elif val > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "food['sentiment_dummies'] = food['sentiment_score'].apply(dummy_maker)\n",
    "\n",
    "# food['sentiment_dummies_v2'] = food['sentiment_score_v2'].apply(dummy_maker)\n",
    "\n",
    "# food['sentiment_dummies_uncleaned'] = food['text_sentiment_no_clean'].apply(dummy_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    61842\n",
      " 1    59187\n",
      "-1    22949\n",
      "Name: sentiment_dummies, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print(food['sentiment_dummies_v1'].value_counts()) \n",
    "# print(food['sentiment_dummies_v2'].value_counts())\n",
    "print(food['sentiment_dummies'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Wendys: The mixtape drops now. Not pulling punches. We Beefin’. https://t.co/H1Rm1ODYC4\n",
      "\n",
      "\n",
      "[1]\n",
      "Some businesses supporting #MarchForOurLives through either donations, discounts or percentage of proceeds, accordi… https://t.co/8xDcgzMQjW\n",
      "\n",
      "\n",
      "[0]\n",
      "RT @DennysDiner: a denny’s haiku \n",
      "\n",
      "if you need a bath,\n",
      "but you’ve got no hot water,\n",
      "just use warm gravy.\n",
      "\n",
      "\n",
      "[-1]\n",
      "@TMobile Tuesdays been slacking. What happened to my free @lyft credits and @dunkindonuts credit i dont want no free t-mobile umbrella fam\n",
      "\n",
      "\n",
      "[0]\n",
      "@Starbucks @jeonggukupdates @SugasHero Omg. Is this real HAHAHA I LOVE THIS\n",
      "\n",
      "\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#manually testing reliability of the different measures with a random subset\n",
    "random_numbers = list(np.random.randint(0, 62415, 5))\n",
    "for num, val in enumerate(food.iloc[random_numbers, :]['text']):\n",
    "    print(val)\n",
    "    print('\\n')\n",
    "    print(food.iloc[[num], :]['sentiment_dummies'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to quickly separate positive/negative tweets by company\n",
    "\n",
    "def negativity_formatter(company, hourly_rate=False):\n",
    "    \"\"\"This function accepts a company and returns either \n",
    "    the separate company, positive, negative dataframes (in that order)\n",
    "    OR the same order plus a dataframe of hourly rates, if hourly_rate = True\"\"\"\n",
    "    df = food.loc[food['name'] == company]\n",
    "    positive_df = df.loc[df['sentiment_dummies'] == 1]\n",
    "    negative_df = df.loc[df['sentiment_dummies'] == -1]\n",
    "    if not hourly_rate:\n",
    "        return df, positive_df, negative_df\n",
    "    else:\n",
    "        rate_df = pd.DataFrame()\n",
    "        rate_df['pos_count'] = positive_df.groupby(positive_df['time_tweeted'].dt.hour)['Company'].count()\n",
    "        rate_df['neg_count'] = negative_df.groupby(negative_df['time_tweeted'].dt.hour)['Company'].count()\n",
    "        rate_df['rate'] = rate_df['neg_count'] / (rate_df['pos_count'] + rate_df['neg_count'])\n",
    "        return rate_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4290655516815069"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#43% of tweets are retweets\n",
    "food.loc[food['text'].str[:2] == 'RT'].shape[0] / food.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a dummy for whether the tweet is a retweet or not\n",
    "retweets = []\n",
    "for val in food['text']:\n",
    "    if val[:2] == 'RT':\n",
    "        retweets.append(1)\n",
    "    else: \n",
    "        retweets.append(0)\n",
    "food['is_a_retweet'] = retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting rid of tweets without a user \n",
    "food = food.drop(list(food.loc[(food['user_name'].isnull())].index), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping regional affiliates\n",
    "associated_comps = []\n",
    "names = []\n",
    "for num, name in enumerate(food['user_name']):\n",
    "    test = re.findall(r'\\A(mcdon|dunki|redro|starbuc|shakesh|domino|crackerb)[a-z]+'\n",
    "                      , name, flags=re.IGNORECASE)\n",
    "    if test:\n",
    "        if name not in ['Dunkin Fails', 'Dunkin Kitti', \"McDonald's employee\"]:\n",
    "            associated_comps.append(num)\n",
    "            names.append(name)\n",
    "            \n",
    "food = food.drop(associated_comps, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning to look at user profiles by company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#customer profiles still skewed by large means- looking at medians & means\n",
    "#creating a subset to examine the numeric characteristics of each company's customers\n",
    "individual_users = food.drop_duplicates(subset='user_name')\n",
    "\n",
    "customer_numeric_df = individual_users.groupby('name').agg({'favorite_count': ['mean', 'max'], \n",
    "                                      'number_of_people_they_follow': ['median', 'mean'],\n",
    "                                      'number_of_user_tweets': ['median', 'mean'],\n",
    "                                      'retweet_count': ['median', 'mean'],\n",
    "                                      'user_followers_count': ['median', 'mean'],\n",
    "                                       'user_is_verified': ['mean'],\n",
    "                                      'sentiment_score': ['mean'], \n",
    "                                      'is_a_retweet': 'mean',\n",
    "                                      'Company': 'count'})\n",
    "\n",
    "customer_numeric_df.columns = [' '.join(col).strip() for col in customer_numeric_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Not enough observations for Potbelly (684), RedRobin (1463), dropping\n",
    "food = food.drop(list(food.loc[(food['Company']== '@redrobinburgers')|\n",
    "                   (food['Company']== '@Potbelly')].index), axis=0)\n",
    "food = food.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MISSING VALUES FOR CERTAIN COMPANIES AT CERTAIN HOURS- hourly is flawed\n",
    "# negativity_by_comp = pd.DataFrame()\n",
    "# for val in food.name.unique().tolist():\n",
    "#     if val != 'McDonalds':\n",
    "#         values = list(negativity_formatter(val, hourly_rate=True)['rate'].values)\n",
    "#         negativity_by_comp[val] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food['day_date'] = food['time_tweeted'].dt.day\n",
    "food['weekday'] = food['time_tweeted'].dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning look at stock movements and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specifying trading days with sufficient information range\n",
    "stock_analysis = food.loc[(food['time_tweeted'] > pd.to_datetime('2018.03.11')) & (food['time_tweeted'] < pd.to_datetime('2018.03.31'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66552, 21)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_analysis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = pd.read_csv('./twitter_stocks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks.drop(['High', 'Low', 'Adj Close'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks['change'] = stocks['Close'] - stocks['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating datetime, and locating common trading dates\n",
    "stocks['Date'] = pd.to_datetime(stocks.Date)\n",
    "stocks['day'] = stocks['Date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = stocks.loc[(stocks['day'] > 11) & (stocks['day'] < 31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making sure values match before merging\n",
    "stocks['Name'] = stocks.Name.str.replace('Shack Shack', 'Shake_Shack')\n",
    "stocks['Name'] = stocks.Name.str.replace('Cracker Barrel', 'Cracker_Barrel')\n",
    "stocks['Name'] = stocks.Name.str.replace('Dunkin Donuts', 'Dunkin_Donuts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grouping sentiments to merge\n",
    "grouped_analysis = stock_analysis.groupby(['day_date', 'name'], as_index=False).agg(\n",
    "                                                    {'sentiment_score':'mean',\n",
    "                                                       'Company':'count'})\n",
    "\n",
    "grouped_analysis.columns =  ['day_date', 'name', 'sentiment_score', 'num_observations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_stock = pd.merge(grouped_analysis, stocks, left_on=['day_date', 'name'], right_on=['day', 'Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping redundant columns \n",
    "merged_stock.drop(['day', 'Name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_stock = merged_stock.sort_values(['name', 'day_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "day_date           -0.094365\n",
       "sentiment_score     1.000000\n",
       "num_observations   -0.013644\n",
       "Open               -0.014959\n",
       "Close              -0.012543\n",
       "Volume              0.145016\n",
       "Market Cap          0.089305\n",
       "change              0.102586\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#correlations between sentiment_score and various other indicators (volume/change/market_cap)\n",
    "merged_stock.corr()['sentiment_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0412179413740274"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating change in sentiment day-to-day by company\n",
    "merged_stock['diff_sent'] = merged_stock.groupby(['name'])['sentiment_score'].transform(lambda x: x.diff())\n",
    "\n",
    "#correlation in sentiment/change in price across companies \n",
    "#this run for change in sentiment/price (with no shift) was insignificant\n",
    "correlations = []\n",
    "for comp in merged_stock.name.unique().tolist():\n",
    "    x = merged_stock.loc[merged_stock['name'] == comp]\n",
    "    correlations.append(x.corr().loc['sentiment_score', 'Volume'])\n",
    "np.mean(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment correlations lagged one day: -0.05092106518166626\n"
     ]
    }
   ],
   "source": [
    "#same as above with different lagged sentiments\n",
    "#sentiment lagged 1 day\n",
    "merged_stock['diff_sent'] = merged_stock.groupby(['name'])['sentiment_score'].transform(lambda x: x.diff()).shift(-1)\n",
    "\n",
    "#day to day correlation in change in sentiment/price across companies \n",
    "correlations = []\n",
    "for comp in merged_stock.name.unique().tolist():\n",
    "    x = merged_stock.copy()\n",
    "    x = x.loc[merged_stock['name'] == comp]\n",
    "    x.diff_sent = x.diff_sent.shift(-1)\n",
    "    x = x[:-1]\n",
    "    correlations.append(x.corr().loc['diff_sent', 'change'])\n",
    "\n",
    "print('sentiment correlations lagged one day:', np.mean(correlations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock changes lagged one day correlation 0.14710532159554887\n"
     ]
    }
   ],
   "source": [
    "#same as above with different lagged sentiments\n",
    "#sentiment lagged 1 day\n",
    "merged_stock['diff_sent'] = merged_stock.groupby(['name'])['sentiment_score'].transform(lambda x: x.diff())\n",
    "\n",
    "#day to day correlation in change in sentiment/price across companies \n",
    "correlations = []\n",
    "\n",
    "for comp in merged_stock.name.unique().tolist():\n",
    "    x = merged_stock.copy()\n",
    "    x = x.loc[merged_stock['name'] == comp]\n",
    "    x.diff_sent = x.diff_sent.shift(1)\n",
    "    x = x[1:]\n",
    "    correlations.append(x.corr().loc['diff_sent', 'change'])\n",
    "\n",
    "print('stock changes lagged one day correlation', np.mean(correlations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    }
   ],
   "source": [
    "#how many mentions of stock or market there are\n",
    "stock = []\n",
    "market = []\n",
    "for ind, val in enumerate(stock_analysis.text_sentiment):\n",
    "    x = re.search(r'(stock)', val, re.IGNORECASE)\n",
    "    y = re.search(r'(market)',val, re.IGNORECASE)\n",
    "    if x:\n",
    "        stock.append(ind)\n",
    "    if y:\n",
    "        market.append(ind)\n",
    "print(len(set(stock + market)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['day_date', 'name', 'sentiment_score', 'num_observations', 'Date',\n",
       "       'Open', 'Close', 'Volume', 'Market Cap', 'change', 'diff_sent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_stock.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value:  0.27\n"
     ]
    }
   ],
   "source": [
    "#permutation from direct correlation w/o lag, not change in sentiment\n",
    "x = merged_stock['sentiment_score']\n",
    "y = merged_stock['change']\n",
    "\n",
    "correlations = np.empty(1000)\n",
    "\n",
    "for i in range(1000):\n",
    "    corrs = []\n",
    "    for comp in merged_stock.name.unique().tolist():\n",
    "        x = merged_stock.copy()\n",
    "        x = x.loc[merged_stock['name'] == comp]\n",
    "        perms = np.random.permutation(x['Volume'])\n",
    "        corrs.append(stats.pearsonr(perms, x['sentiment_score'])[0])\n",
    "    correlations[i] = np.mean(corrs)\n",
    "    \n",
    "print('p_value: ', np.sum(correlations > .0509) / len(correlations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value:  0.0\n"
     ]
    }
   ],
   "source": [
    "##p-value from a permutation test for day lag \n",
    "correlations = np.empty(1000)\n",
    "\n",
    "for i in range(1000):\n",
    "    corrs = []\n",
    "    for comp in merged_stock.name.unique().tolist():\n",
    "        x = merged_stock.copy()\n",
    "        x = x.loc[x['name'] == comp]\n",
    "        x.diff_sent = x.diff_sent.shift(1)\n",
    "        x = x[1:]\n",
    "        corrs.append(x.corr().loc['diff_sent', 'change'])\n",
    "    correlations[i] = np.mean(corrs)\n",
    "    \n",
    "print('p_value: ', np.sum(correlations > .149) / len(correlations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Profiles of negative/positive sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set of english vocabulary\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lang_composition(company):\n",
    "    \"\"\"Takes a company name and returns the total number of words present, \n",
    "    the number of unique words present, and the number of those words that are in the English\n",
    "    language\"\"\"\n",
    "    all_tweets, positive, negative = negativity_formatter(company)\n",
    "    all_text = ' '\n",
    "    pos_text = ' '\n",
    "    neg_text = ' '\n",
    "    for val in all_tweets.text:\n",
    "        if val[:2] != 'RT':\n",
    "            all_text = all_text + ' ' + val\n",
    "    for val in positive.text:\n",
    "        if val[:2] != 'RT':\n",
    "            pos_text = pos_text + ' ' + val\n",
    "    for val in negative.text:\n",
    "        if val[:2] != 'RT':\n",
    "            neg_text = neg_text + ' ' + val\n",
    "    all_words = Text_Cleaner(all_text, tokens = True)\n",
    "    pos_words = Text_Cleaner(pos_text, tokens = True)\n",
    "    neg_words = Text_Cleaner(neg_text, tokens = True)\n",
    "    info = [company]\n",
    "    info.extend([len(all_words), len(pos_words), len(neg_words)])\n",
    "    info.extend([len(set(all_words)), len(set(pos_words)), len(set(neg_words))])\n",
    "    all_clean_words = [word for word in set(all_words) if word in english_vocab]\n",
    "    pos_clean_words = [word for word in set(pos_words) if word in english_vocab]\n",
    "    neg_clean_words = [word for word in set(neg_words) if word in english_vocab]\n",
    "    info.extend([len(all_clean_words), len(pos_clean_words), len(neg_clean_words)])\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting language composition for all companies\n",
    "all_comps = []\n",
    "for name in list(food.name.unique()):\n",
    "    all_comps.append(lang_composition(name))\n",
    "    \n",
    "lang_comp_df = pd.DataFrame(all_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#renaming all columns\n",
    "lang_comp_df.columns = ['name', 'all_words','pos_all_words', 'neg_all_words', 'unique_words', 'pos_unique_words', 'neg_unique_words', \n",
    "                        'english_words', 'pos_english_words', 'neg_english_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating percentages for analysis\n",
    "lang_comp_df['percent_unique'] = lang_comp_df['unique_words'] / lang_comp_df['all_words'] * 100\n",
    "lang_comp_df['percent_english'] = lang_comp_df['english_words'] / lang_comp_df['unique_words'] * 100\n",
    "lang_comp_df['pos_percent_unique'] = lang_comp_df['pos_unique_words'] / lang_comp_df['pos_all_words'] * 100\n",
    "lang_comp_df['pos_percent_english'] = lang_comp_df['pos_english_words'] / lang_comp_df['pos_unique_words'] * 100\n",
    "lang_comp_df['neg_percent_unique'] = lang_comp_df['neg_unique_words'] / lang_comp_df['neg_all_words'] * 100\n",
    "lang_comp_df['neg_percent_english'] = lang_comp_df['neg_english_words'] / lang_comp_df['neg_unique_words'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merging sentiments \n",
    "lang_comp_df = pd.merge(lang_comp_df, food.groupby('name', as_index=False)['sentiment_score'].mean(), \n",
    "       on='name')\n",
    "\n",
    "#Merging only negative sentiments \n",
    "food_neg = food.loc[food['sentiment_dummies'] == -1].groupby('name',as_index=False)['sentiment_score'].mean()\n",
    "food_neg.columns = ['name', 'neg_sentiment']\n",
    "lang_comp_df = pd.merge(lang_comp_df, food_neg, on='name')\n",
    "\n",
    "#Merging only positive sentiments\n",
    "food_pos = food.loc[food['sentiment_dummies'] == 1].groupby('name',as_index=False)['sentiment_score'].mean()\n",
    "food_neg.columns = ['name', 'pos_sentiment']\n",
    "lang_comp_df = pd.merge(lang_comp_df, food_pos, on='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_words             -0.555627\n",
       "pos_all_words         -0.425108\n",
       "neg_all_words         -0.668694\n",
       "unique_words          -0.489475\n",
       "pos_unique_words      -0.383168\n",
       "neg_unique_words      -0.607556\n",
       "english_words         -0.442953\n",
       "pos_english_words     -0.356219\n",
       "neg_english_words     -0.548201\n",
       "percent_unique         0.289452\n",
       "percent_english        0.498901\n",
       "pos_percent_unique     0.121678\n",
       "pos_percent_english    0.354084\n",
       "neg_percent_unique     0.548569\n",
       "neg_percent_english    0.714317\n",
       "sentiment_score_x      1.000000\n",
       "neg_sentiment          0.784287\n",
       "sentiment_score_y      0.635131\n",
       "Name: sentiment_score_x, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#strongest correlations with overall sentiment- percent unique (.31), percent English (0.29)\n",
    "lang_comp_df.corr()['sentiment_score_x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Looking at company tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_tweets = pd.read_csv('./company_tweets.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19760, 11)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_tweets['time_tweeted'] = pd.to_datetime(company_tweets['time_tweeted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#there are no duplicates\n",
    "company_tweets.unique_code.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19760, 11)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping potbelly & redrobin\n",
    "company_tweets.drop(list(company_tweets.loc[(company_tweets['name'] == '@redrobinburgers')\n",
    "                  |(company_tweets['name'] == '@Potbelly')].index), axis=0,\n",
    "                   inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting normally spelled names\n",
    "mapper = {'@DennysDiner': 'Dennys', '@ChipotleTweets': 'Chipotle',\n",
    "         '@McDonalds': 'McDonalds', '@Wendys': 'Wendys', '@Starbucks':'Starbucks',\n",
    "         '@dunkindonuts':'Dunkin_Donuts', '@dominos': 'Dominos', '@shakeshack': 'Shake_Shack',\n",
    "         '@sonicdrivein': 'Sonic', '@wingstop': 'Wingstop', '@CrackerBarrel': 'Cracker_Barrel', \n",
    "         '@redrobinburgers': 'Red_Robin', '@Potbelly': 'Potbelly'}\n",
    "\n",
    "company_tweets['Company'] = company_tweets.name.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-04-19 16:48:37')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food.time_tweeted.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_tweets.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping all company tweets that occurred after consumer tweet collection stopped\n",
    "company_tweets.drop(list(\n",
    "    company_tweets.loc[company_tweets['time_tweeted'] > food.time_tweeted.max()].index), axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16720, 12)\n"
     ]
    }
   ],
   "source": [
    "print(company_tweets.shape)\n",
    "company_tweets = company_tweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping values that are before consumer observations\n",
    "company_tweets.drop(list(company_tweets.loc[company_tweets['time_tweeted'] < \n",
    "                                       food.time_tweeted.min()].index), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_tweets = company_tweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 167.51it/s]\n"
     ]
    }
   ],
   "source": [
    "#Getting the hashtags for each company\n",
    "hashtags = {}\n",
    "callnames = {}\n",
    "for company in tqdm(list(company_tweets.Company.unique())): \n",
    "    df = company_tweets.loc[company_tweets['Company'] == company]\n",
    "    hash_base = []\n",
    "    call_base = []\n",
    "    for val in df['text']:\n",
    "        hashes = re.findall(r'#[A-Za-z0-9]+\\b', val)\n",
    "        calls = re.findall(r'@[A-Za-z0-9]+\\b', val)\n",
    "        hash_base.extend(hashes)\n",
    "        call_base.extend(calls)\n",
    "    hashtags[company] = hash_base\n",
    "    callnames[company] = call_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'#[A-Za-z0-9]+\\b')\n",
    "#Only interested in customers independently using the hashtags\n",
    "hash_check = food.copy()\n",
    "hash_check = hash_check.loc[food['is_a_retweet'] == 0]\n",
    "hash_check['text'] = hash_check.text.apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Identifying customer use of company hashtags\n",
    "\n",
    "company_counts = {}\n",
    "for company in list(company_tweets.Company.unique()):\n",
    "    hash_comp = hash_check.loc[hash_check['name'] == company]\n",
    "    hashes = {}\n",
    "    for val in hash_comp['text']:\n",
    "        for word in val:\n",
    "            if word in hashtags[company]:\n",
    "                try: \n",
    "                    hashes[word] += 1\n",
    "                except:\n",
    "                    hashes[word] = 1\n",
    "    company_counts[company] = hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#putting statistics on hashtags/use into new company profile dataframe\n",
    "new_df = pd.DataFrame(company_counts).T.sum(axis=1).reset_index()\n",
    "new_df.columns = ['name', 'customer_hash_use']\n",
    "new_df['customer_unique_hashes'] = pd.DataFrame(company_counts).T.count(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#examining company specific twitter behavior\n",
    "comp_hash_uses = []\n",
    "comp_unique_hashes = []\n",
    "comp_handle_uses = []\n",
    "comp_unique_handle_uses = []\n",
    "for company in new_df.name.tolist():\n",
    "    comp_hash_uses.append(len(hashtags[company]))\n",
    "    comp_unique_hashes.append(len(set(hashtags[company])))\n",
    "    comp_handle_uses.append(len(callnames[company]))\n",
    "    comp_unique_handle_uses.append(len(set(callnames[company])))\n",
    "\n",
    "new_df['comp_hash_uses'] = comp_hash_uses\n",
    "new_df['comp_unique_hashes'] = comp_unique_hashes\n",
    "new_df['comp_handle_uses'] = comp_handle_uses\n",
    "new_df['comp_unique_handle_uses'] = comp_unique_handle_uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grouping company tweet info\n",
    "comp_grouped = company_tweets.groupby('Company', as_index=False).mean().drop(\n",
    "                            ['is_a_retweet', 'is_quote_status', 'unique_code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_merged = pd.merge(comp_grouped, new_df, left_on='Company', right_on='name')\n",
    "comp_merged.drop('name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_merged['market_cap'] = merged_stock.groupby('name')['Market Cap'].mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_merged['sentiment'] = food.groupby('name')['sentiment_score'].mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "@ChipotleTweets    0.999342\n",
       "@CrackerBarrel     0.769536\n",
       "@DennysDiner       0.218382\n",
       "@McDonalds         0.997368\n",
       "@Starbucks         0.974342\n",
       "@Wendys            0.997368\n",
       "@dominos           0.990789\n",
       "@dunkindonuts      0.971711\n",
       "@shakeshack        0.959211\n",
       "@sonicdrivein      0.959211\n",
       "@wingstop          0.984868\n",
       "Name: is_contact, dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Percentage of tweets that are direct customer contact\n",
    "customer_contact = []\n",
    "for val in company_tweets['text']:\n",
    "    if val[0] == '@':\n",
    "        customer_contact.append(1)\n",
    "    else:\n",
    "        customer_contact.append(0)\n",
    "company_tweets['is_contact'] = customer_contact\n",
    "\n",
    "company_tweets.groupby('name')['is_contact'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9031160784790135"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_tweets['is_contact'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding a tweets per follower category to account for size\n",
    "comp_merged['percent_tweets_foll'] = comp_merged['number_of_tweets_total'] / comp_merged['company_followers_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Correlations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "retweet_count             -0.240913\n",
       "comp_unique_handle_uses   -0.237383\n",
       "market_cap                -0.216188\n",
       "number_of_tweets_total    -0.126253\n",
       "followers_count           -0.096115\n",
       "comp_hash_uses            -0.012132\n",
       "customer_unique_hashes     0.372163\n",
       "favorite_count             0.375593\n",
       "comp_handle_uses           0.391963\n",
       "customer_hash_use          0.392710\n",
       "comp_unique_hashes         0.395412\n",
       "percent_tweets_foll        0.441691\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_merged.drop('company_followers_count', axis=1, inplace=True)\n",
    "print('Sentiment Correlations')\n",
    "comp_merged.corr()['sentiment'].sort_values()[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day of week optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ensuring that every weekday has equal representation (Friday will be slightly short-changed (by ~3 hours)\n",
    "day_week = food.loc[(food['time_tweeted'] > '2018-03-09')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting day nums to names, grouping by company and day of week and aggregating. \n",
    "mapper = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\n",
    "day_week = day_week.groupby(['name', 'weekday'], as_index=False).agg({'sentiment_score':'mean', 'user_name': 'count',\n",
    "                                                          'retweet_count':'mean', 'favorite_count':'mean'})\n",
    "\n",
    "day_week['weekday'] = day_week['weekday'].map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#including average sentiment for visualization\n",
    "merge = day_week.groupby('weekday', as_index=False)['sentiment_score'].mean()\n",
    "merge.columns = ['weekday', 'avg_sentiment']\n",
    "day_week = pd.merge(day_week, merge, on='weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets by day\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "weekday\n",
       "Sunday       15335\n",
       "Monday       18728\n",
       "Saturday     19039\n",
       "Friday       20065\n",
       "Wednesday    20959\n",
       "Tuesday      22902\n",
       "Thursday     23041\n",
       "Name: user_name, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('number of tweets by day')\n",
    "day_week.groupby('weekday')['user_name'].sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average sentiment by day\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "weekday\n",
       "Saturday     0.081780\n",
       "Sunday       0.097866\n",
       "Friday       0.100423\n",
       "Monday       0.116630\n",
       "Thursday     0.125735\n",
       "Wednesday    0.129243\n",
       "Tuesday      0.139522\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('average sentiment by day')\n",
    "day_week.groupby('weekday')['sentiment_score'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Beginning profile analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profiles = food.dropna(subset=['user_profile_text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119113, 21)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#only interested in unique profiles\n",
    "profiles = profiles.drop_duplicates(subset='user_name').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82276, 21)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping columns I don't need for this\n",
    "profiles.drop(['user_coordinates', 'unique_code', 'day_date', 'text_sentiment'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding an hour dummy\n",
    "profiles['hour'] = profiles.time_tweeted.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting rid of columns I don't need anymore\n",
    "profiles = profiles.drop(['time_tweeted', 'Company'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Maybe the number of hashtags a person uses is indicative of their personality/disposition towards the product\n",
    "def num_hashes(text):\n",
    "    \"\"\"Counts the number of hashtags in the profile\"\"\"\n",
    "    text = text.split()\n",
    "    num_hashes = 0\n",
    "    for word in text:\n",
    "        if word[0] == '#':\n",
    "            num_hashes = num_hashes + 1\n",
    "    return num_hashes\n",
    "\n",
    "profiles['number_of_hashes'] = profiles['user_profile_text'].apply(num_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08159122952015169"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8% of users use a hashtag in their profile\n",
    "np.sum(profiles.number_of_hashes > 1) / profiles.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding a dummy variable if the user lists a location, possibly indicator\n",
    "profiles['lists_location'] = profiles.user_location.isnull().astype(int)\n",
    "\n",
    "#unreliable for now to include\n",
    "profiles = profiles.drop('user_location', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaning_additional(text):\n",
    "    \"\"\"standardizing contractions, removing urls, removing retweet indicators, removing handles\"\"\"\n",
    "    text = re.sub(r'https[\\S]+', ' ', text)\n",
    "    text = re.sub(r'can\\'t', 'can not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'don\\'t', 'do not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'isn\\'t', 'is not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'aren\\'t', 'are not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'wasn\\'t', 'was not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'weren\\'t', 'were not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'haven\\'t', 'have not', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(rt|RT)', ' ', text)\n",
    "    text = re.sub(r'@[\\S]+', ' ', text)\n",
    "    text = re.sub(r'[0-9]*[a-zA-Z]+[0-9]+[a-zA-Z]*[0-9]*[a-zA-Z]*', ' ', text)\n",
    "    return text\n",
    "\n",
    "profiles['cleaned'] = profiles.user_profile_text.apply(cleaning_additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punctuation_cleaner(text):\n",
    "    \"\"\"Removes punctuation and places spaces\"\"\"\n",
    "    text = re.sub(r'\\\\n', ' ', text)      \n",
    "    text = re.sub(r'[!|?|.|,|(|)|||[|]|/|\\\\|-]', ' ', text)\n",
    "    return text\n",
    "\n",
    "profiles['cleaned'] = profiles.cleaned.apply(punctuation_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_hashes(text):\n",
    "    \"\"\"Turns camel case hashtags into separate readable words\"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    split2 = []\n",
    "    for val in words:\n",
    "        if val[0] == '#':\n",
    "            words = ' '.join(re.findall('[A-Z][^A-Z]*', val))\n",
    "            split2.append(words)\n",
    "        else:\n",
    "            split2.append(val)\n",
    "    return ' '.join(split2)\n",
    "\n",
    "profiles['without_hashes'] = profiles.cleaned.apply(split_hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### T-test for mean difference between sentiment with/without retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=2.3715728048141727, pvalue=0.026886738879467794)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full = [0.065029, 0.094908, 0.097293, 0.101714, 0.106599, 0.108249, 0.113954, 0.121663, 0.147069, 0.153147, 0.162750, 0.173199, 0.184626]\n",
    "no_retweet = [0.089,0.132,0.072,0.0439,0.089,0.055887,0.124,0.089954,0.111304,0.134573,0.084565]\n",
    "\n",
    "stats.ttest_ind(full, no_retweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Looking at possible occupational/regional tendencies with a noun phrase function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words that aren't caught that don't add value to function below\n",
    "drop_words = ['love','don t', 'twitter', 'don', 'tweets', 'opinions', 'your',\n",
    "             'born', 'never']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Profile_Noun_Finder(text):\n",
    "    \"\"\"Takes text, eliminates non-alpha characters, and returns all nouns\"\"\"\n",
    "    text = re.sub(r'(https)[^\\s]+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z]+', ' ', text)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    lowered = [word.lower() for word in words]\n",
    "    words = [word for word in lowered if len(word) > 2]\n",
    "    words = [word for word in words if word not in drop_words]\n",
    "    new_text = ' '.join(words)\n",
    "    wiki = TextBlob(new_text)\n",
    "    return list(wiki.noun_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at data with/without verified users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['favorite_count', 'number_of_people_they_follow',\n",
       "       'number_of_user_tweets', 'retweet_count', 'text',\n",
       "       'user_followers_count', 'user_is_verified', 'user_name',\n",
       "       'user_profile_text', 'name', 'sentiment_score', 'sentiment_dummies',\n",
       "       'is_a_retweet', 'weekday', 'hour', 'number_of_hashes', 'lists_location',\n",
       "       'cleaned', 'without_hashes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "without = profiles.copy()\n",
    "without = without.loc[without['user_is_verified'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with_ver = profiles.copy()\n",
    "with_ver = with_ver.loc[with_ver['user_is_verified'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80638, 19)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02104639098858752"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_ver.shape\n",
    "852 / (39630 + 852)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verified:\n",
      " 0.13658429972302708 \n",
      "                 user_name  sentiment_score\n",
      "name                                      \n",
      "Chipotle              175         0.161718\n",
      "Cracker_Barrel         84         0.126873\n",
      "Dennys                 56         0.132770\n",
      "Dominos               162         0.117590\n",
      "Dunkin_Donuts         297         0.118739\n",
      "McDonalds             188         0.160339\n",
      "Shake_Shack           220         0.140338\n",
      "Sonic                 111         0.121065\n",
      "Starbucks             196         0.163859\n",
      "Wendys                102         0.138000\n",
      "Wingstop               47         0.121135\n",
      "\n",
      "Unverified:\n",
      " 0.11665966878234232 \n",
      "                 user_name  sentiment_score\n",
      "name                                      \n",
      "Chipotle             8708         0.157244\n",
      "Cracker_Barrel       3323         0.147277\n",
      "Dennys               9157         0.095777\n",
      "Dominos              8559         0.062175\n",
      "Dunkin_Donuts        8676         0.099223\n",
      "McDonalds            9601         0.084571\n",
      "Shake_Shack          4242         0.186215\n",
      "Sonic                5283         0.102859\n",
      "Starbucks           10078         0.122661\n",
      "Wendys               9373         0.112357\n",
      "Wingstop             3638         0.112897\n"
     ]
    }
   ],
   "source": [
    "with_ver_x = with_ver.groupby('name').agg({'user_name':'count', 'sentiment_score':'mean'})\n",
    "without_x = without.groupby('name').agg({'user_name':'count', 'sentiment_score':'mean'})\n",
    "print('Verified:\\n',with_ver_x['sentiment_score'].mean(),'\\n', with_ver_x)\n",
    "print('\\nUnverified:\\n',without_x['sentiment_score'].mean(), '\\n', without_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Chipotle</th>\n",
       "      <td>175</td>\n",
       "      <td>0.161718</td>\n",
       "      <td>0.004475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cracker_Barrel</th>\n",
       "      <td>84</td>\n",
       "      <td>0.126873</td>\n",
       "      <td>-0.020403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dennys</th>\n",
       "      <td>56</td>\n",
       "      <td>0.132770</td>\n",
       "      <td>0.036992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominos</th>\n",
       "      <td>162</td>\n",
       "      <td>0.117590</td>\n",
       "      <td>0.055415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dunkin_Donuts</th>\n",
       "      <td>297</td>\n",
       "      <td>0.118739</td>\n",
       "      <td>0.019516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>McDonalds</th>\n",
       "      <td>188</td>\n",
       "      <td>0.160339</td>\n",
       "      <td>0.075768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shake_Shack</th>\n",
       "      <td>220</td>\n",
       "      <td>0.140338</td>\n",
       "      <td>-0.045877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sonic</th>\n",
       "      <td>111</td>\n",
       "      <td>0.121065</td>\n",
       "      <td>0.018206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Starbucks</th>\n",
       "      <td>196</td>\n",
       "      <td>0.163859</td>\n",
       "      <td>0.041198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wendys</th>\n",
       "      <td>102</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.025643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wingstop</th>\n",
       "      <td>47</td>\n",
       "      <td>0.121135</td>\n",
       "      <td>0.008238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                user_name  sentiment_score  difference\n",
       "name                                                  \n",
       "Chipotle              175         0.161718    0.004475\n",
       "Cracker_Barrel         84         0.126873   -0.020403\n",
       "Dennys                 56         0.132770    0.036992\n",
       "Dominos               162         0.117590    0.055415\n",
       "Dunkin_Donuts         297         0.118739    0.019516\n",
       "McDonalds             188         0.160339    0.075768\n",
       "Shake_Shack           220         0.140338   -0.045877\n",
       "Sonic                 111         0.121065    0.018206\n",
       "Starbucks             196         0.163859    0.041198\n",
       "Wendys                102         0.138000    0.025643\n",
       "Wingstop               47         0.121135    0.008238"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_ver_x['difference'] = with_ver_x['sentiment_score'] - without_x['sentiment_score']\n",
    "with_ver_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# food.to_csv('./complete_data_models.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
